{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Note - Azure Machine Learning Introduction and Designer Interface\n",
    "This notebook collects notes taken during the course **[Introduction to the Azure Machine Learning SDK](https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-machine-learning-service/)** and **[Create no-code predictive models with Azure Machine Learning](https://docs.microsoft.com/en-us/learn/paths/create-no-code-predictive-models-azure-machine-learning/)** offered by Microsoft, as well as documentations of Azure from Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning tools and interfaces\n",
    "\n",
    "Azure Machine Learning (Azure ML) is a **cloud-based service** for **creating and managing machine learning solutions**. It's designed to help data scientists leverage their existing data processing and model development skills and frameworks, and help them **scale their workloads to the cloud**. The Azure ML SDK for Python provides classes you can use to work with Azure ML in your Azure subscription.\n",
    "\n",
    "### Tools\n",
    "#### Azure Machine Learning studio\n",
    "A web-based tool for managing an Azure Machine Learning workspace. It enables you to create, manage, and view all of the assets in your workspace and provides the following graphical tools:\n",
    "- Designer\n",
    "- Automated Machine Learning\n",
    "\n",
    "#### The Azure Machine Learning SDK for Python\n",
    "- Documentation: https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py \n",
    "\n",
    "#### SDK for R\n",
    "#### Visual Studio Code extension\n",
    "#### Machine learning CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Using Azure Machine Learning: Workspace -> Experiment -> Run\n",
    "### [Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace)\n",
    "The workspace is ***the top-level resource*** for Azure Machine Learning, providing **a centralized place to work with all the artifacts you create when you use Azure Machine Learning**.\n",
    "\n",
    "#### Machine learning tasks read and/or write artifacts to your workspace.\n",
    "\n",
    "- **Run an experiment** to train a model - writes experiment run results to the workspace.\n",
    "- Use **automated ML** to train a model - writes training results to the workspace.\n",
    "- **Register a model** in the workspace.\n",
    "- **Deploy a model** - uses the registered model to create a deployment.\n",
    "- Create and run **reusable workflows**.\n",
    "- **View machine learning artifacts** such as experiments, pipelines, models, deployments.\n",
    "- Track and **monitor models**.\n",
    "\n",
    "\n",
    "### Run an Experiment\n",
    "\n",
    "In Azure Machine Learning, an experiment is **a named process**, usually **the running of a script or a pipeline**, that can generate metrics and outputs and be tracked in the Azure Machine Learning workspace.\n",
    "\n",
    "An experiment can be run multiple times, with different data, code, or settings; and Azure Machine Learning tracks each run, enabling you to view run history and compare results for each run.\n",
    "\n",
    "When you submit an experiment, you use its run context to initialize and end the experiment run that is tracked in Azure Machine Learning, as shown in the following code sample:\n",
    "from azureml.core import Experiment\n",
    "```python\n",
    "# create an experiment variable\n",
    "experiment = Experiment(workspace = ws, name = \"my-experiment\")\n",
    "\n",
    "# start the experiment\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# experiment code goes here\n",
    "\n",
    "# end the experiment\n",
    "run.complete()\n",
    "```\n",
    "\n",
    "<ins>Running a Script as an Experiment</ins>\n",
    "\n",
    "You can run an experiment inline using the start_logging method of the Experiment object, but it's more common to encapsulate the experiment logic in a script and run the script as an experiment.\n",
    "\n",
    "To access the experiment run context (which is needed to log metrics) the script must import the **azureml.core.Run** class and call its **get_context** method. The script can then use the run context to log metrics, upload files, and complete the experiment, as shown in the following example:\n",
    "```python\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Count the rows and log the result\n",
    "row_count = (len(data))\n",
    "run.log('observations', row_count)\n",
    "\n",
    "# Save a sample of the data\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "data.sample(100).to_csv(\"outputs/sample.csv\", index=False, header=True)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()\n",
    "```\n",
    "\n",
    "To run a script as an experiment, you must define \n",
    "- a *run configuration* that defines the Python environment in which the script will be run, and \n",
    "- a *script run configuration* that associates the run environment with the script. \n",
    "These are implemented by using the **RunConfiguration** and **ScriptRunConfig** objects.\n",
    "\n",
    "For example, the following code could be used to run an experiment based on a script in the **experiment_files** folder (which must also contain any files used by the script, such as the data.csv file in previous script code example):\n",
    "\n",
    "```python\n",
    "from azureml.core import Experiment, RunConfiguration, ScriptRunConfig\n",
    "\n",
    "# create a new RunConfig object\n",
    "experiment_run_config = RunConfiguration()\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder, \n",
    "                                script='experiment.py',\n",
    "                                run_config=experiment_run_config) \n",
    "\n",
    "# submit the experiment\n",
    "experiment = Experiment(workspace = ws, name = 'my-experiment')\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output=True)\n",
    "```\n",
    "\n",
    "The **RunConfiguration** object defines the Python environment for the experiment, including the packages available to the script. If your script depends on packages that are not included in the default environment, you must associate the **RunConfiguration** with an Environment object that makes use of a **CondaDependencies** object to specify the Python packages required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Machine Learning Designer\n",
    "-\tScore Data module is like `.predict()` function in Python packages\n",
    "\n",
    "### Set up\n",
    "#### 1. Create an Azure Machine Learning workspace in *Azure Portal*\n",
    "\n",
    "To use Azure Machine Learning, you create a workspace in your Azure subscription. You can then use this workspace to manage data, compute resources, code, models, and other artifacts related to your machine learning workloads.\n",
    "\n",
    "#### 2. Go to *Azure Machine Learning studio*, Link the workspace, and build models from there\n",
    "\n",
    "#### 3. Create compute target on *Azure Machine Learning studio*\n",
    "\n",
    "After you have created an Azure Machine Learning workspace, you can use it to manage the various assets and resources you need to create machine learning solutions.\n",
    "\n",
    "There are four kinds of compute resource:\n",
    "- **Compute Instances**: **Development workstations** that data scientists can use to work with data and models.\n",
    "- **Compute Clusters**: **Scalable clusters** of virtual machines **for on-demand processing of experiment code**.\n",
    "- **Inference Clusters**: **Deployment targets** for predictive services that use your trained models.\n",
    "- **Attached Compute**: Links to existing Azure compute resources, such as Virtual Machines or Azure Databricks clusters.\n",
    "\n",
    "We'll create a compute target that consists both **compute instances** and **compute clusters** for development.\n",
    "\n",
    "### Use automated machine learning\n",
    "\n",
    "Azure Machine Learning includes an **automated machine learning capability** that leverages the scalability of cloud compute to automatically try multiple pre-processing techniques and model-training algorithms in parallel to find the best performing supervised machine learning model for your data.\n",
    "\n",
    "In Azure Machine Learning studio, view the Automated ML page (under Author). To create a new Automated ML run:\n",
    "\n",
    "1. Select dataset\n",
    "2. Configure run\n",
    "3. Task type and settings\n",
    "\n",
    "### Deploy a predictive service\n",
    "\n",
    "In Azure Machine Learning, you can deploy a service as an Azure Container Instances (ACI) or to an Azure Kubernetes Service (AKS) cluster. For production scenarios, an AKS deployment is recommended, for which you must create an ***inference cluster*** compute target.\n",
    "\n",
    "In Azure Machine Learning studio, select the base model and deploy.\n",
    "\n",
    "To use the deployed model, you need the following information:\n",
    "- The REST endpoint for your service\n",
    "- the Primary Key for your service\n",
    "\n",
    "Run a notebook with the following codes in a seperate Azure Machine learning studio.\n",
    "\n",
    "```python\n",
    "endpoint = 'YOUR_ENDPOINT' #Replace with your endpoint\n",
    "key = 'YOUR_KEY' #Replace with your key\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "#An array of features based on five-day weather forecast\n",
    "x = [[1,1,2022,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446],\n",
    "    [2,1,2022,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539]]\n",
    "\n",
    "#Convert the array to JSON format\n",
    "input_json = json.dumps({\"data\": x})\n",
    "\n",
    "#Set the content type and authentication for the request\n",
    "headers = {\"Content-Type\":\"application/json\",\n",
    "        \"Authorization\":\"Bearer \" + key}\n",
    "\n",
    "#Send the request\n",
    "response = requests.post(endpoint, input_json, headers=headers)\n",
    "\n",
    "#If we got a valid response, display the predictions\n",
    "if response.status_code == 200:\n",
    "    y = json.loads(response.json())\n",
    "    print(\"Predictions:\")\n",
    "    for i in range(len(x)):\n",
    "        print (\" Day: {}. Predicted rentals: {}\".format(i+1, max(0, round(y[\"result\"][i]))))\n",
    "else:\n",
    "    print(response)\n",
    "```\n",
    "\n",
    "### The process of building ML workflow on Azure\n",
    "\n",
    "#### 1. Set up Workspace and Compute Target\n",
    "\n",
    "#### 2. Create a training pipeline \n",
    "\n",
    "In Azure Machine Learning studio, view the Designer page (under Author), and select + to create a new pipeline.\n",
    "\n",
    "1. Add and explore a datase\n",
    "\n",
    "2. Add data transformation tasks as neccessary\n",
    "\n",
    "3. Add steps to train and evaluate model with training and test set\n",
    "\n",
    "4. Run the training pipeline and find out the best model\n",
    "\n",
    "Select Submit, and run the pipeline using the existing experiment named auto-price-training.\n",
    "\n",
    "#### 3. Create an inference pipeline\n",
    "\n",
    "After creating and running a pipeline to train the model, you need a second pipeline that performs the same data transformations for new data, and then uses the trained model to **inference (in other words, predict) label values** based on its features. This will form the basis for a predictive service that you can publish for applications to use.\n",
    "\n",
    "1. In Azure Machine Learning Studio, click the **Designer** page to view all of the pipelines you have created. Then open the  **Auto Price Training** pipeline you created previously.\n",
    "\n",
    "2. In the **Create inference pipeline** drop-down list, click **Real-time inference pipeline**. After a few seconds, a new version of your pipeline named **Auto Price Training-real time inference** will be opened.\n",
    "\n",
    "3. Rename the new pipeline to Predict Auto Price, and then review the new pipeline. **It contains a *web service input* for new data to be submitted, and a *web service output* to return results.** Some of the transformations and training steps have been encapsulated in this pipeline so that the statistics from your training data will be used to normalize any new data values, and the trained model will be used to score the new data. Make the following changes:\n",
    "    - (For testing) Replace training dataset with an **Enter Data Manually** module that does not include the lable column.\n",
    "    - Modify the **Select Columns in Dataset** modeul to remove any reference to the label column.\n",
    "    - Remove the **Evaluate Model** module.\n",
    "    - Insert an **Execute Python Script** module before the web service output to **return only the predicted label**.\n",
    "\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "\n",
    "    def azureml_main(dataframe1 = None, dataframe2 = None):\n",
    "\n",
    "        scored_results = dataframe1[['Scored Labels']]\n",
    "        scored_results.rename(columns={'Scored Labels':'predicted_price'},\n",
    "                            inplace=True)\n",
    "        return scored_results\n",
    "    ```\n",
    "\n",
    "4. Submit the pipeline as a new experiment\n",
    "\n",
    "#### 4. Deploy a predictive service\n",
    "\n",
    "To publish a **real-time inference pipeline** as a service, you must deploy it to an **Azure Kubernetes Service (AKS) cluster**.\n",
    "\n",
    "- Go to the inference pipeline and select **Deploy** on the top right.\n",
    "- Set up a new real-time endpoint on the cluster\n",
    "- Get the REST endpoint and Primary Key of the service\n",
    "- Use the REST endpoint and Primary Key to connect to application that make predictions on new data (Set up on client side)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
