{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Note - Building AI Solutions with Azure Machine Learning\n",
    "This notebook collects the notes taken through the course of **[Build AI solutions with Azure Machine Learning](https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/)** offered by Microsoft, with supplements from the **[documentation of Azure Machine Learning SDK for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py)**.\n",
    "\n",
    "This notebook contains Labs 06 - 07 of the learning course, which correspond to \"Deploy and Consume Models\" section in the exam guideline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Deploy real-time machine learning services with Azure Machine Learning\n",
    "In machine learning, *inferencing* refers to the use of a trained model to predict labels for new data on which the model has not been trained. In Azure Machine learning, you can create **real-time inferencing solutions by deploying a model as a service**, hosted in a **containerized platform** such as **Azure Kubernetes Services (AKS)**.\n",
    "\n",
    "You can deploy a model as a real-time web service to several kinds of compute target, including \n",
    "1. local compute, \n",
    "2. an Azure Machine Learning compute instance, \n",
    "3. an Azure Container Instance (ACI), \n",
    "4. an Azure Kubernetes Service (AKS) cluster, \n",
    "5. an Azure Function, or \n",
    "6. an Internet of Things (IoT) module.\n",
    "\n",
    "Azure Machine Learning uses **containers** as a deployment mechanism, packaging the model and the code to use it as an image that can be deployed to a container in your chosen compute target.\n",
    "\n",
    "**Notes:** \n",
    "- **We can also deploy the model on Azure Container Instances (ACI) Web Service or local Docker-based service during development and testing.**\n",
    "- **ACI web service is best for small scale testing and quick deployments, and AKS is for deloyments as a production-scale web service.**\n",
    "\n",
    "### Tasks to deploy a model as a real-time inferencing service\n",
    "#### 1. Register a trained model\n",
    "```python\n",
    "# Approach 1: Register method of Model object\n",
    "from azureml.core import Model\n",
    "\n",
    "classification_model = Model.register(workspace=ws,\n",
    "                       model_name='classification_model',\n",
    "                       model_path='model.pkl', # local path\n",
    "                       description='A classification model')\n",
    "\n",
    "# Approach 2: register_model mothod of Run object\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model')\n",
    "```\n",
    "\n",
    "#### 2. Define an inference configuration\n",
    "\n",
    "The model will be deployed as a service that consist of:\n",
    "- **A script** to load the model and return predictions for submitted data.\n",
    "- **An environment** in which the script will be run.\n",
    "You must therefore define the script and environment for the service.\n",
    "\n",
    "##### 2.1.\tCreate an **entry script**: The entry script receives data submitted to a deployed web service and passes it to the model. It then takes the response returned by the model and returns that to the client. *The script is specific to your model.* It must understand the data that the model expects and returns.\n",
    "1.\t`init()`: Called when the service is initialized - Typically, this function loads the model into a global object. This function is run only once, when the Docker container for your web service is started.\n",
    "2.\t`run(inpute_data)`: Called with new data is submitted to the service - This function uses the model to predict a value based on the input data. Inputs and outputs of the run typically use JSON for serialization and deserialization. You can also work with raw binary data. You can transform the data before sending it to the model or before returning it to the client.\n",
    "\n",
    "*Typically, you use the **init** function to **load the model** from the model registry, and use the **run** function to **generate predictions from the input data**.* The following example script shows this pattern:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    # Get the path to the registered model file and load it\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    # Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # Get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # Return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()\n",
    "```\n",
    "        \n",
    "##### 2.2. Create an environment\n",
    "```python\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Add the dependencies for your model\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "\n",
    "# Save the environment config as a .yml file\n",
    "env_file = 'service_files/env.yml'\n",
    "with open(env_file,\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Saved dependency info in\", env_file)\n",
    "```\n",
    "\n",
    "##### 2.3. Combine the script and environment in an InferenceConfig\n",
    "\n",
    "```python\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                              source_directory = 'service_files',\n",
    "                                              entry_script=\"score.py\",\n",
    "                                              conda_file=\"env.yml\")\n",
    "```\n",
    "\n",
    "#### 3.\tDefine a deployment configuration on the chosen compute target\n",
    "Now that you have the entry script and environment, you need to **configure the compute** to which the service will be deployed.\n",
    "\n",
    "- AksCompute\n",
    "```python\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name = 'aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "production_cluster.wait_for_completion(show_output=True)\n",
    "```\n",
    "\n",
    "With the compute target created, you can now define the deployment configuration, which sets the target-specific compute specification for the containerized deployment:\n",
    "\n",
    "```python\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                              memory_gb = 1)\n",
    "```\n",
    "\n",
    "The code to configure an ACI deployment is similar, except that you do not need to explicitly create an ACI compute target, and you must use the **deploy_configuration** class from the **azureml.core.webservice.AciWebservice** namespace. Similarly, you can use the **azureml.core.webservice.LocalWebservice** namespace to configure a local Docker-based service.\n",
    "\n",
    "- ACI deployment\n",
    "```python\n",
    "from azureml.core.webservice import AciWebservice\n",
    "```\n",
    "\n",
    "- local Docker-based service\n",
    "```python\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "```\n",
    "\n",
    "#### 4.\tDeploy the model\n",
    "```python\n",
    "# Use deploy method of the Model class\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model], #1. Model registered\n",
    "                       inference_config = classifier_inference_config, # 2. Inference Configuration\n",
    "                       deployment_config = classifier_deploy_config, # 3. deployment configuration\n",
    "                       deployment_target = production_cluster) # (Optional) 3. deployment configuration\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)\n",
    "```\n",
    "\n",
    "To delete a deployed web service, use `service.delete()`. To delete a registered model, use `model.delete()`.\n",
    "\n",
    "### Consume a real-time inferencing service\n",
    "\n",
    "After deploying a real-time service, you can consume it from client applications to predict labels for new data cases.\n",
    "\n",
    "#### Using the Azure Machine Learning SDK\n",
    "\n",
    "For testing, you can use the Azure Machine Learning SDK to call a web service through the run method of a WebService object that references the deployed service. Typically, you send data to the run method in JSON format with the following structure:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "  \"data\":[\n",
    "      [0.1,2.3,4.1,2.0], // 1st case\n",
    "      [0.2,1.8,3.9,2.1],  // 2nd case,\n",
    "      ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "```Python\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "response = service.run(input_data = json_data)\n",
    "\n",
    "# Get the predictions\n",
    "predictions = json.loads(response)\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i], predictions[i])\n",
    "```\n",
    "\n",
    "\n",
    "#### Using a REST Endpoint\n",
    "\n",
    "In production, most client applications will not include the Azure Machine Learning SDK, and will consume the service through its REST interface. You can determine the endpoint of a deployed service in Azure machine Learning studio, or by retrieving the scoring_uri property of the Webservice object in the SDK.\n",
    "\n",
    "```python\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)\n",
    "```\n",
    "\n",
    "With the endpoint known, you can use an HTTP POST request with JSON data to call the service.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint, # put endpoint here\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i], predictions[i])\n",
    "```\n",
    "\n",
    "#### Authentication\n",
    "\n",
    "In production, you will likely want to restrict access to your services by applying authentication. There are two kinds of authentication you can use:\n",
    "\n",
    "- **Key**: Requests are authenticated by specifying the key associated with the service.\n",
    "- **Token**: Requests are authenticated by providing a JSON Web Token (JWT).\n",
    "\n",
    "Assuming you have an authenticated session established with the workspace, you can retrieve the keys for a service by using the get_keys method of the WebService object associated with the service:\n",
    "\n",
    "```python\n",
    "primary_key, secondary_key = service.get_keys()\n",
    "```\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# An array of new data cases\n",
    "x_new = [[0.1,2.3,4.1,2.0],\n",
    "         [0.2,1.8,3.9,2.1]]\n",
    "\n",
    "# Convert the array to a serializable list in a JSON document\n",
    "json_data = json.dumps({\"data\": x_new})\n",
    "\n",
    "# Set the content type in the request headers\n",
    "request_headers = { \"Content-Type\":\"application/json\",\n",
    "                    \"Authorization\":\"Bearer \" + key_or_token } # add key here\n",
    "\n",
    "# Call the service\n",
    "response = requests.post(url = endpoint,\n",
    "                         data = json_data,\n",
    "                         headers = request_headers)\n",
    "\n",
    "# Get the predictions from the JSON response\n",
    "predictions = json.loads(response.json())\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "for i in range(len(x_new)):\n",
    "    print (x_new[i], predictions[i] )\n",
    "```\n",
    "\n",
    "\n",
    "### [Additional Topic: Create an endpoint](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service#create-an-endpoint)\n",
    "\n",
    "**To create an endpoint, use `AksEndpoint.deploy_configuration` instead of `AksWebservice.deploy_configuration()`.**\n",
    "\n",
    "```python\n",
    "import azureml.core,\n",
    "from azureml.core.webservice import AksEndpoint\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "# select a created compute\n",
    "compute = ComputeTarget(ws, 'myaks')\n",
    "namespace_name= endpointnamespace\n",
    "# define the endpoint and version name\n",
    "endpoint_name = \"mynewendpoint\"\n",
    "version_name= \"versiona\"\n",
    "# create the deployment config and define the scoring traffic percentile for the first deployment\n",
    "endpoint_deployment_config = AksEndpoint.deploy_configuration(cpu_cores = 0.1, memory_gb = 0.2,\n",
    "                                                              enable_app_insights = True,\n",
    "                                                              tags = {'sckitlearn':'demo'},\n",
    "                                                              description = \"testing versions\",\n",
    "                                                              version_name = version_name,\n",
    "                                                              traffic_percentile = 20)\n",
    " # deploy the model and endpoint\n",
    "endpoint = Model.deploy(ws, endpoint_name, [model], inference_config, endpoint_deployment_config, compute)\n",
    " # Wait for he process to complete\n",
    "endpoint.wait_for_deployment(True)\n",
    "```\n",
    "\n",
    "To *consume* a deployed real-time service (or model or endpoint), we’ll need the following: **(Note: Recall the consume tab in AML Studio.)**\n",
    "-\tHTTP Post/ Url **(Note: Recall the step to copy the REST url on AML studio and paste it in the script.)**\n",
    "-\tKey **(Note: Recall the step to copy the primary key on AML studio and paste it in the script.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 Deploy batch inference pipelines with Azure Machine Learning\n",
    "\n",
    "In many production scenarios, long-running tasks that operate on large volumes of data are performed as *batch operations*. In machine learning, *batch inferencing* is used to apply a predictive model to multiple cases asynchronously - usually writing the results to a file or database.\n",
    "\n",
    "In Azure Machine Learning, you can implement batch inferencing solutions by creating a pipeline that includes a step to **read the input data, load a registered model, predict labels, and write the results as its output**.\n",
    "\n",
    "### Creating a batch inference pipeline\n",
    "\n",
    "The steps are not very consistent between lectures and lab codes. Refer to the lab codes when there’s inconsistency.\n",
    "\n",
    "#### 1.\tRegister a model\n",
    "#### 2.\tCreate a scoring script and define a run context that includes the dependencies required by the script\n",
    "```python\n",
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "def init():\n",
    "    # Runs when the pipeline step is initialized\n",
    "    global model\n",
    "\n",
    "    # load the model\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    resultList = []\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        # Read comma-delimited data into an array\n",
    "        data = np.genfromtxt(f, delimiter=',')\n",
    "        # Reshape into a 2-dimensional array for model input\n",
    "        prediction = model.predict(data.reshape(1, -1))\n",
    "        # Append prediction to results\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    return resultList\n",
    "```\n",
    "\n",
    "#### 3.\tCreate a pipeline with **ParallelRunStep**\n",
    "\n",
    "Azure Machine Learning provides a type of pipeline step specifically for performing parallel batch inferencing. Using the **ParallelRunStep** class, you can read batches of files from a File dataset and write the processing output to a **PipelineData** reference. Additionally, you can set the **output_action** setting for the step to \"append_row\", which will ensure that all instances of the step being run in parallel will collate their results to a single output file named parallel_run_step.txt.\n",
    "\n",
    "```python\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Get the batch dataset for input\n",
    "batch_data_set = ws.datasets['batch-data']\n",
    "\n",
    "# Set the output location\n",
    "default_ds = ws.get_default_datastore()\n",
    "output_dir = PipelineData(name='inferences',\n",
    "                          datastore=default_ds,\n",
    "                          output_path_on_compute='results')\n",
    "\n",
    "# Define the parallel run step step configuration\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='batch_scripts',\n",
    "    entry_script=\"batch_scoring_script.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=aml_cluster,\n",
    "    node_count=4)\n",
    "\n",
    "# Create the parallel run step\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('batch_data')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "```\n",
    "#### 4.\tRun the pipeline and retrieve the step output\n",
    "\n",
    "After your pipeline has been defined, you can run it and wait for it to complete. Then you can retrieve the **parallel_run_step.txt** file from the output of the step to view the results\n",
    "\n",
    "```python\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Run the pipeline as an experiment\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)\n",
    "\n",
    "# Get the outputs from the first (and only) step\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='results')\n",
    "\n",
    "# Find the parallel_run_step.txt file\n",
    "for root, dirs, files in os.walk('results'):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# Load and display the results\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Publishing a batch inference pipeline\n",
    "\n",
    "```python\n",
    "# Publish a batch inferencing pipeline as a REST service\n",
    "published_pipeline = pipeline_run.publish_pipeline(name='Batch_Prediction_Pipeline',\n",
    "                                                   description='Batch pipeline',\n",
    "                                                   version='1.0')\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "\n",
    "\n",
    "# Use the service endpoint to initiate a batch inferencing job\n",
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"Batch_Prediction\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "\n",
    "# Schedule the published pipeline to have it run automatically\n",
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "weekly = ScheduleRecurrence(frequency='Week', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Weekly Predictions',\n",
    "                                        description='batch inferencing',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Batch_Prediction',\n",
    "                                        recurrence=weekly)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
