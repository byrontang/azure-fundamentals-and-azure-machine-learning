{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Note - Building AI Solutions with Azure Machine Learning\n",
    "This notebook collects the notes taken through the course of **[Build AI solutions with Azure Machine Learning](https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/)** offered by Microsoft, with supplements from the **[documentation of Azure Machine Learning SDK for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py)**.\n",
    "\n",
    "This notebook contains Labs 01 - 05 of the learning course, which correspond to \"Set up an Azure Machine Learning Workspace\" and \"Run Experiments and Train Models\" sections in the exam guideline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Getting Started with Azure Machine Learning\n",
    "\n",
    "The Azure ML SDK for Python provides classes you can use to work with Azure ML in your Azure subscription.\n",
    "\n",
    "### azureml-core package\n",
    "**High level process:**\n",
    "1. **create a new <font color='blue'>*workspace*</font> or connect to an existing workspace** \n",
    "2. **create an Azure ML <font color='blue'>*experiment*</font> in workspace**\n",
    "3. **create a <font color='blue'>*run*</font> to run codes**\n",
    "\n",
    "### Workspace \n",
    "\n",
    "A **workspace** is a context for the **experiments, data, compute targets, and other assets** associated with **a machine learning workload**. Workspaces are Azure resources, and as such they are defined within a resource group in an Azure subscription, along with other related Azure resources that are required to support the workspace. A Workspace is a fundamental **resource** for machine learning in Azure Machine Learning. You use a workspace to **experiment, train, and deploy machine learning models**.\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "```\n",
    "- All experiments and associated resources are managed within you Azure ML workspace. You can connect to an existing workspace,  create a new one using the Azure ML SDK, or load the workspace from the configuration file.\n",
    "\n",
    "```python\n",
    "# Load an existing workspace\n",
    "ws = Workspace.get(name=\"myworkspace\", subscription_id='<azure-subscription-id>', resource_group='myresourcegroup')\n",
    "\n",
    "# Create a new one\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='<azure-subscription-id>',\n",
    "                      resource_group='myresourcegroup',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2'\n",
    "                     )\n",
    "\n",
    "# Load from a configuration file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "```\n",
    "\n",
    "- In most cases, you should store the workspace configuration in a JSON configuration file. This makes it easier to reconnect without needing to remember details like your Azure subscription ID.\n",
    "\n",
    "```python\n",
    "ws.write_config(path=\"./file-path\", file_name=\"ws_config.json\")\n",
    "```\n",
    "\n",
    "- You can download the JSON configuration file from the blade for your workspace in the Azure portal, but ***if you're using a Compute Instance within your workspace, the configuration file has already been downloaded to the root folder.***\n",
    "    - ***Note: It means if the new script is on the same compute instance, you can simply use `Workspace.from_comfig()` to retrieve workspace.***\n",
    "- `.from_config()` finds and uses the configuration file from the root folder to connect to your workspace.\n",
    "\n",
    "```python\n",
    "ws_other_environment = Workspace.from_config(path=\"./file-path/ws_config.json\")\n",
    "```\n",
    "\n",
    "### Experiment\n",
    "\n",
    "In Azure Machine Learning, an **experiment** is a **named process**, usually the running of a script or a pipeline, that can generate metrics and outputs and be tracked in the Azure Machine Learning workspace. An experiment can be run multiple times, with different data, code, or settings; and Azure Machine Learning tracks each run, enabling you to view run history and compare results for each run.\n",
    "\n",
    "When you submit an experiment, you use its run context to initialize and end the experiment run that is tracked in Azure Machine Learning\n",
    "\n",
    "```python\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# create an experiment variable\n",
    "experiment = Experiment(workspace=ws, name='test-experiment')\n",
    "\n",
    "# start the experimennt\n",
    "run = experiment.start_logging()\n",
    "\n",
    "# experiment code goes here\n",
    "\n",
    "# end the experiment\n",
    "run.complete()\n",
    "\n",
    "```\n",
    "\n",
    "After the experiment run has completed, you can view the details of the run in the **Experiments** tab in Azure Machine Learning studio.\n",
    "\n",
    "### Run\n",
    "**A run represent a single trial of an experiment.** **Run** is the object that you use to monitor the asynchronous execution of a trial, store the output of the trial, analyze results, and access generated artifacts. You use Run inside your experimentation code to log metrics and artifacts to the Run History service.\n",
    "\n",
    "#### Note: *Run* is like running a model/pipeline each time.\n",
    "\n",
    "- There are two ways to create run. Both functions return a Run object.\n",
    "    1. `experiment.start_logging()` as previous example\n",
    "    2. `experiment.submit()` to run a experiment script \n",
    "\n",
    "#### [IMPORTANT!] If you're interactively experimenting in a Jupyter notebook, use the `start_logging` function. If you're submitting an experiment from a standard Python environment, use the `submit` function. \n",
    "\n",
    "Create a Run object by submitting an Experiment object with a **run configuration object**. Use the **tags parameter** to attach custom categories and labels to your runs. You can easily find and retrieve them later from Experiment.\n",
    "\n",
    "```python\n",
    "tags = {\"prod\": \"phase-1-model-tests\"}\n",
    "run = experiment.submit(config=your_config_object, tags=tags)\n",
    "```\n",
    "\n",
    "#### Create an experiemnt script\n",
    "- Create a separate script from experiment, store it in a folder along with any other files it needs, and then use Azure ML to run the experiment based on the script in the folder.\n",
    "- `Run.get_context()` method to *retrieve the experiment run context when the script is run*.\n",
    "- <ins>**After a run object is created, use various `.log*()` methods to log the outputs.**</ins>\n",
    "- `run.complete()` at the end of the script\n",
    "\n",
    "```python\n",
    "# An experiment script, experiment.py, saved in the experiment_files folder\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Count the rows and log the result\n",
    "row_count = (len(data))\n",
    "run.log('observations', row_count)\n",
    "\n",
    "# Save a sample of the data\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "data.sample(100).to_csv(\"outputs/sample.csv\", index=False, header=True)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()\n",
    "```\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "To run a script as an experiment, you must define a script configuration that defines **the script to be run** and **the Python environment in which to run it**. This is implemented by using a **ScriptRunConfig** object.\n",
    "\n",
    "```python\n",
    "from azureml.core import Experiment, RunConfiguration, ScriptRunConfig\n",
    "\n",
    "# create a new RunConfig object\n",
    "experiment_run_config = RunConfiguration()\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder, \n",
    "                                script='experiment.py',\n",
    "                                run_config=experiment_run_config) \n",
    "\n",
    "# submit the experiment\n",
    "experiment = Experiment(workspace = ws, name = 'my-experiment')\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output=True)\n",
    "```\n",
    "\n",
    "The **RunConfiguration** object defines the Python environment for the experiment, including the packages available to the script. If your script depends on packages that are not included in the default environment, you must associate the **RunConfiguration** with an Environment object that makes use of a **CondaDependencies** object to specify the Python packages required.\n",
    "\n",
    "### Note: How to create a simple machine learning workflow\n",
    "\n",
    "1. Create a new workspace or load an exsiting workspace\n",
    "2. Create experiment script and save it in the folder along with other files\n",
    "3. Configure the file and submit the experiment\n",
    "\n",
    "### [Lab: Getting Started with Azure Machine Learning](https://github.com/MicrosoftDocs/mslearn-aml-labs/blob/master/labdocs/Lab01.md)\n",
    "\n",
    "- In this lab, we need to create a **workspace** in Azure Portal and then use **Azure Machine Learning studio** to manage the workspace.\n",
    "    - Create a compute instance under the workspace. ***When creating a Compute Instance, a virtual machine is created.***\n",
    "    - The cheapest virtual machine is STANDARD_D2S_V3\n",
    "        - After the compute instance is created, click its **Jupyter link** to open Jupyter Notebooks on the VM.\n",
    "- **[IMPORTANT!!]** When you have finished the lab, **close all Jupyter tabs and *Stop* your compute instance** to avoid incurring unnecessary costs.\n",
    "\n",
    "### MLflow\n",
    "\n",
    "**MLflow** is an open source platform for managing machine learning processes. It's **commonly (but not exclusively) used in Databricks environments** to coordinate experiments and track metrics. In Azure Machine Learning experiments, you can use MLflow to track metrics instead of the native log functionality if you desire.\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "```\n",
    "- Refer to the notebook codes in official Git-Hub fore more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Training Models with Parameters\n",
    "\n",
    "In Azure Machine Learning, you can use a **Run Configuration** and a **Script Run Configuration** to run a script-based experiment that trains a machine learning model. However, depending on the machine learning framework being used and **the dependencies** it requires, **the run configuration may become complex**.\n",
    "\n",
    "Azure Machine Learning also provides a higher level abstraction called an **Estimator** that ***encapsulates a run configuration and a script configuration*** in a single object, and for which there are pre-defined, framework-specific variants that already include the package dependencies for common machine learning frameworks such as *Scikit-Learn, PyTorch, and Tensorflow*.\n",
    "\n",
    "#### Note: \n",
    "- A difference is to replace script_config with estimator (create estimator object and pass it into the config parameter)\n",
    "- The rest of process to run a model with experiments is basically the same. \n",
    "\n",
    "### Steps:\n",
    "#### Create a training script and log key metrics of modeling performance\n",
    "#### Run the script as experiment\n",
    "- Option 1: Use an Estimator\n",
    "\n",
    "```python\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an estimator\n",
    "estimator = Estimator(source_directory='experiment_folder',\n",
    "                      entry_script='training_script.py',\n",
    "                      compute_target='local',\n",
    "                      conda_packages=['scikit-learn']\n",
    "                      )\n",
    "\n",
    "# Create and run an experiment\n",
    "experiment = Experiment(workspace = ws, name = 'training_experiment')\n",
    "run = experiment.submit(config=estimator) # Note here the estimator is passed to the config parameter\n",
    "```\n",
    "\n",
    "- Option 2: using framewrk-specific estimators\n",
    "\n",
    "```python\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an estimator\n",
    "estimator = SKLearn(source_directory='experiment_folder',\n",
    "                    entry_script='training_script.py'\n",
    "                    compute_target='local'\n",
    "                    )\n",
    "\n",
    "# Create and run an experiment\n",
    "experiment = Experiment(workspace = ws, name = 'training_experiment')\n",
    "run = experiment.submit(config=estimator) # Note here the estimator is passed to the config parameter\n",
    "```\n",
    "\n",
    "#### Register the trained model to the workspace\n",
    "\n",
    "Note that **the outputs of the experiment include the trained model file (model.pkl)**. You can register this model in your Azure Machine Learning workspace, making it possible to track model versions and retrieve them later.\n",
    "\n",
    "Model registration enables you to track multiple versions of a model, and retrieve models for ***inferencing (predicting label values from new data)***. When you register a model, you can specify a name, description, tags, framework (such as Scikit-Learn or PyTorch), framework version, custom properties, and other useful metadata. Registering a model with the same name as an existing model automatically creates a new version of the model, starting with 1 and increasing in units of 1.\n",
    "\n",
    "- Option 1: **register** method of **Model** object\n",
    "\n",
    "```python\n",
    "from azureml.core import Model\n",
    "\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_name='classification_model',\n",
    "                       model_path='model.pkl', # local path\n",
    "                       description='A classification model',\n",
    "                       tags={'dept': 'sales'},\n",
    "                       model_framework=Model.Framework.SCIKITLEARN,\n",
    "                       model_framework_version='0.20.3')\n",
    "```\n",
    "\n",
    "- Option 2: reference to the **Run**\n",
    "\n",
    "```python\n",
    "run.register_model( model_name='classification_model',\n",
    "                    model_path='outputs/model.pkl', # run outputs path\n",
    "                    description='A classification model',\n",
    "                    tags={'dept': 'sales'},\n",
    "                    model_framework=Model.Framework.SCIKITLEARN,\n",
    "                    model_framework_version='0.20.3')\n",
    "```\n",
    "\n",
    "#### Viewing registered models\n",
    "```python\n",
    "from azureml.core import Model\n",
    "\n",
    "for model in Model.list(ws):\n",
    "    # Get model name and auto-generated version\n",
    "    print(model.name, 'version:', model.version)\n",
    "```\n",
    "    \n",
    "\n",
    "### Also: using script parameters\n",
    "\n",
    "#### Add argument into script\n",
    "Adding parameters to your script enables you to repeat the same training experiment with different settings\n",
    "To use parameters in a script, you must use a library such as **argparse** to read the arguments passed to the script and assign them to variables.\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "# also import other packages as neccessary\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Set regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "\n",
    "# Prepare the dataset\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# The rest of the script\n",
    "```\n",
    "\n",
    "#### Passing Script Arguments to an Estimator\n",
    "```python\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Configure/create an estimator\n",
    "estimator = SKLearn(source_directory='experiment_folder',\n",
    "                    entry_script='training_script.py',\n",
    "                    script_params = {'--reg_rate': 0.1},\n",
    "                    compute_target='local'\n",
    "                    )\n",
    "\n",
    "# Create and run an experiment\n",
    "experiment = Experiment(workspace = ws, name = 'training_experiment')\n",
    "run = experiment.submit(config=estimator)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: Revisit how to interprete ROC\n",
    "- Y axis calculates True Positive Rate – the base is True (Ex: 80 True instances)\n",
    "- X axis calculates False Positive Rate – the base is False (Ex: 20 False instances)\n",
    "    - If we select True by randomly, the probability of selecting a true or false instance is 0.8 and 0.2. Therefore, TPR and FPR will increase at around the same pace.\n",
    "    - However, if we build a good predictive model, the probability of selecting a true instance should increase, skewing the curve to the top-left. ***The better the capability of the model to predict true positive, the higher the AUC.***\n",
    "\n",
    "#### Don’t confuse the concept of AUC and Accuracy.\n",
    "- AUC shows **the capability of a model to predict true positives**, and each axis has different base.\n",
    "- The base of accuracy includes both true and false instances. It doesn’t take into account the capability of predicting true positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Work with Data in Azure Machine Learning\n",
    "\n",
    "### [IMPORTANT NOTE] Datastores are *file locations* whereas datasets are are *real data*.\n",
    "\n",
    "### Datastores\n",
    "In Azure Machine Learning, ***datastores*** are abstractions for cloud data sources / storage locations.\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Register a new datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(workspace=ws,\n",
    "    datastore_name='blob_data',\n",
    "    container_name='data_container',\n",
    "    account_name='az_store_acct',\n",
    "    account_key='123456abcde789…')    \n",
    "\n",
    "# Get reference to a data score\n",
    "blob_store = Datastore.get(ws, datastore_name='blob_data')\n",
    "default_store = ws.get_default_datastore()\n",
    "ws.set_default_datastore('blob_data')\n",
    "\n",
    "# Working directly with a datastore\n",
    "blob_ds.upload(src_dir='/files',\n",
    "               target_path='/data/files',\n",
    "               overwrite=True, show_progress=True)\n",
    "\n",
    "blob_ds.download(target_path='downloads',\n",
    "                 prefix='/data',\n",
    "                 show_progress=True)\n",
    "```\n",
    "\n",
    "When you want to use a datastore in an experiment script, you must pass a data reference to the script. The data reference is configured for one of the following data access modes: **download, upload, and mount.**\n",
    "\n",
    "```python\n",
    "# Get a data reference\n",
    "data_ref = blob_ds.path('data/files').as_download(path_on_compute='training_data')\n",
    "\n",
    "# Configuration\n",
    "estimator = SKLearn(source_directory='experiment_folder',\n",
    "                    entry_script='training_script.py'\n",
    "                    compute_target='local',\n",
    "                    script_params = {'--data_folder': data_ref})\n",
    "```\n",
    "\n",
    "In your training script, you can retrieve the parameter and use it like a local folder:\n",
    "```python\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', type=str, dest='data_folder')\n",
    "args = parser.parse_args()\n",
    "data_files = os.listdir(args.data_folder)\n",
    "```\n",
    "\n",
    "### Datasets\n",
    "***Datasets*** are versioned packaged data objects that can be easily consumed in experiments and pipelines. Datasets are the recommended way to work with data, and are the primary mechanism for advanced Azure Machine Learning capabilities like data labeling and data drift monitoring.\n",
    "\n",
    "Datasets are typically based on **files in a datastore**, though they can also be based on URLs and other sources. You can create the following types of dataset: **tabular and file**.\n",
    "\n",
    "```python\n",
    "# Create - Type 1: Creating and registering tabular datasets\n",
    "from azureml.core import Dataset\n",
    "\n",
    "blob_ds = ws.get_default_datastore()\n",
    "\n",
    "    # The dataset in this example includes data from two file paths within the default datastore\n",
    "csv_paths = [(blob_ds, 'data/files/current_data.csv'),\n",
    "             (blob_ds, 'data/files/archive/*.csv')]\n",
    "tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)\n",
    "\n",
    "    # After creating the dataset, the code registers it in the workspace with the name csv_table.\n",
    "tab_ds = tab_ds.register(workspace=ws, name='csv_table')\n",
    "\n",
    "# Create - Type 2: Creating and registering file datasets\n",
    "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files')\n",
    "\n",
    "# Retrieve a registered dataset\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "    # Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "    # Get a dataset from the workspace datasets collection (dictionary attribute)\n",
    "ds1 = ws.datasets['csv_table']\n",
    "\n",
    "    # Get a dataset by name from the datasets class (method)\n",
    "ds2 = Dataset.get_by_name(ws, 'img_files')\n",
    "\n",
    "# Dataset versioning - specifying the create_new_version property\n",
    "img_paths = [(blob_ds, 'data/files/images/*.jpg'),\n",
    "             (blob_ds, 'data/files/images/*.png')]\n",
    "file_ds = Dataset.File.from_files(path=img_paths)\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)\n",
    "\n",
    "# Retrieving a specific dataset version\n",
    "img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)\n",
    "```\n",
    "\n",
    "You can read data directly from a dataset, or you can pass a dataset as a named input to a script configuration or estimator.\n",
    "```python\n",
    "# Working with a dataset directly\n",
    "    # Tabuler\n",
    "df = tab_ds.to_pandas_dataframe()\n",
    "# code to work with dataframe goes here\n",
    "\n",
    "    # File\n",
    "for file_path in file_ds.to_path():\n",
    "    print(file_path)\n",
    "```\n",
    "\n",
    "When you need to access a dataset in an experiment script, you can pass the dataset as an input to a **ScriptRunConfig** or an **Estimator**. For example, the following code passes a tabular dataset to an estimator:\n",
    "\n",
    "Since the script will need to work with a Dataset object, you must include either **the full azureml-sdk package** or **the azureml-dataprep package with the pandas extra library** in the script's compute environment.\n",
    "\n",
    "```python\n",
    "estimator = SKLearn( source_directory='experiment_folder',\n",
    "                     entry_script='training_script.py',\n",
    "                     compute_target='local',\n",
    "                     inputs=[tab_ds.as_named_input('csv_data')],\n",
    "                     pip_packages=['azureml-dataprep[pandas]')\n",
    "```\n",
    "\n",
    "In the experiment script itself, you can access the input and work with the Dataset object it references like this:\n",
    "\n",
    "```python\n",
    "run = Run.get_context()\n",
    "data = run.input_datasets['csv_data'].to_pandas_dataframe()\n",
    "```\n",
    "\n",
    "When passing a file dataset, you must **specify the access mode**. For large volumes of data, you'd generally use the **as_mount** method to stream the files directly from the dataset source; but when running on local compute (as we are in this example), you need to use the **as_download** option to download the dataset files to a local folder.\n",
    "\n",
    "```python\n",
    "estimator = Estimator( source_directory='experiment_folder',\n",
    "                     entry_script='training_script.py'\n",
    "                     compute_target='local',\n",
    "                     inputs=[img_ds.as_named_input('img_data').as_download(path_on_compute='data')],\n",
    "                     pip_packages=['azureml-dataprep[pandas]')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Work with Compute in Azure machine Learning\n",
    "The runtime context for each experiment run consists of two elements:\n",
    "1. The *environment* for the script, which includes all packages used in the script.\n",
    "2. The *compute target* on which the environment will be deployed and the script run. This could be the local workstation from which the experiment run is initiated, or a remote compute target such as a training cluster that is provisioned on-demand.\n",
    "    - In Azure Machine Learning, *Compute Targets* are **physical or virtual computers on which experiments are run**.\n",
    "\n",
    "### Environments in Azure Machine Learning\n",
    "In general, Azure Machine Learning handles environment creation and package installation for you - usually through the creation of **Docker containers**. In addition to Python, you can also configure PySpark, Docker and R for environments. Internally, environments result in **Docker images** that are used to run the training and scoring processes on the compute target.\n",
    "\n",
    "When you run a Python script as an experiment in Azure Machine Learning, a Conda environment is created to define the execution context for the script. Azure Machine Learning provides a default environment that includes many common packages; including the **azureml-defaults** package that contains the libraries necessary for working with an experiment run, as well as popular packages like **pandas** and **numpy**.\n",
    "\n",
    "You can also define your own environment and add packages by using **conda** or **pip**, to ensure your experiment has access to all the libraries it requires.\n",
    "\n",
    "You can have Azure Machine Learning manage environment creation and package installation to define an environment, and then register it for reuse. Alternatively, you can manage your own environments and register them. This makes it possible to define consistent, reusable runtime contexts for your experiments - regardless of where the experiment script is run.\n",
    "\n",
    "```python\n",
    "from azureml.core import Environment\n",
    "\n",
    "# Create an environment\n",
    "# Approach 1: Creating an environment from a specification file\n",
    "env = Environment.from_conda_specification(name='training_environment',\n",
    "                                           file_path='./conda.yml')\n",
    "\n",
    "# Approach 2: Creating an environment from an existing Conda environment\n",
    "env = Environment.from_existing_conda_environment(name='training_environment',\n",
    "                                                  conda_environment_name='py_env')\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# Approach 3: Creating an environment by specifying packages\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "    # Create a Python environment for the experiment\n",
    "diabetes_env = Environment(\"diabetes-experiment-env\")\n",
    "diabetes_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "diabetes_env.docker.enabled = True # Use a docker container\n",
    "\n",
    "    # Create a set of package dependencies (conda or pip as required)\n",
    "diabetes_packages = CondaDependencies.create(conda_packages=['scikit-learn'],\n",
    "                                          pip_packages=['azureml-defaults', 'azureml-dataprep[pandas]'])\n",
    "\n",
    "    # Add the dependencies to the environment\n",
    "diabetes_env.python.conda_dependencies = diabetes_packages\n",
    "```\n",
    "\n",
    "```python\n",
    "# Register an environment\n",
    "env.register(workspace=ws)\n",
    "\n",
    "    # View registered environment\n",
    "env_names = Environment.list(workspace=ws)\n",
    "for env_name in env_names:\n",
    "    print('Name:',env_name)\n",
    "```\n",
    "```python\n",
    "# Retrieving and using an environment\n",
    "from azureml.core import Environment\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "training_env = Environment.get(workspace=ws, name='training_environment')\n",
    "estimator = Estimator(source_directory='experiment_folder'\n",
    "                      entry_script='training_script.py',\n",
    "                      compute_target='local',\n",
    "                      environment_definition=training_env)\n",
    "```\n",
    "\n",
    "### Compute Targets\n",
    "\n",
    "*In Azure Machine Learning, **Compute Targets** are physical or virtual computers on which experiments are run.* Azure Machine Learning supports multiple types of compute for experimentation and training, and for production inferencing. This enables you to select the most appropriate type of compute target for your particular needs.\n",
    "\n",
    "#### Local compute\n",
    "\n",
    "This runs the experiment on the same compute target as the code used to initiate the experiment, which may be your physical workstation or a virtual machine such as an Azure Machine Learning **compute instance on which you are running a notebook**.\n",
    "\n",
    "#### Compute cluster\n",
    "\n",
    "For experiment workloads with high scalability requirements, you can use Azure Machine Learning compute clusters; which are **multi-node clusters of Virtual Machines** that automatically scale up or down to meet demand. This is a cost-effective way to run experiments that need to handle large volumes of data or use parallel processing to distribute the workload and reduce the time it takes to run.\n",
    "\n",
    "#### Inference clusters \n",
    "\n",
    "To deploy trained models as production services, you can use Azure Machine Learning inference clusters, which use **containerization technologies** to enable rapid initialization of compute for on-demand inferencing.\n",
    "\n",
    "#### Attached compute\n",
    "\n",
    "If you already use an Azure-based compute environment for data science, such as a virtual machine or an Azure Databricks cluster, you can attach it to your Azure Machine Learning workspace and use it as a compute target for certain types of workload.\n",
    "\n",
    "#### Sample codes\n",
    "```python\n",
    "# Creating a managed compute target with the SDK\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "# 1. Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# 2. Specify a name for the compute (unique within the workspace)\n",
    "compute_name = 'aml-cluster'\n",
    "\n",
    "# 3. Define compute configuration\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                                       min_nodes=0, max_nodes=4,\n",
    "                                                       vm_priority='dedicated')\n",
    "\n",
    "# 4. Create the compute\n",
    "aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "aml_cluster.wait_for_completion(show_output=True)\n",
    "```\n",
    "\n",
    "In this example, a cluster with up to four nodes that is based on the STANDARD_DS12_v2 virtual machine image will be created. The priority for the virtual machines (VMs) is set to dedicated, meaning they are reserved for use in this cluster (the alternative is to specify lowpriority, which has a lower cost but means that the VMs can be preempted if a higher-priority workload requires the compute).\n",
    "\n",
    "\n",
    "```python\n",
    "# Attaching an unmanaged compute target with the SDK\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
    "\n",
    "# 1. Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# 2. Specify a name for the compute (unique within the workspace)\n",
    "compute_name = 'db_cluster'\n",
    "\n",
    "# 3. Define configuration for existing Azure Databricks cluster\n",
    "db_workspace_name = 'db_workspace'\n",
    "db_resource_group = 'db_resource_group'\n",
    "db_access_token = '1234-abc-5678-defg-90...'\n",
    "db_config = DatabricksCompute.attach_configuration(resource_group=db_resource_group,\n",
    "                                                   workspace_name=db_workspace_name,\n",
    "                                                   access_token=db_access_token)\n",
    "\n",
    "# 4. Create the compute\n",
    "databricks_compute = ComputeTarget.attach(ws, compute_name, db_config)\n",
    "databricks_compute.wait_for_completion(True)\n",
    "```\n",
    "\n",
    "After you've created environments and compute targets in your workspace, you can use them to run specific workloads; such as experiments.\n",
    "\n",
    "When an experiment for the estimator is submitted, the run will be queued while the compute target is started and the specified environment deployed to it, and then the run will be processed on the compute environment.\n",
    "\n",
    "\n",
    "```python\n",
    "estimator = Estimator(source_directory='experiment_folder',\n",
    "                      entry_script='training_script.py',\n",
    "                      environment_definition=training_env,\n",
    "                      compute_target=training_cluster # compute target - specify a new or an object\n",
    "                      )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 Orchestra machine learning with pipelines\n",
    "\n",
    "### Definition\n",
    "The term pipeline is used extensively in machine learning, often with different meanings.\n",
    "- Scikit-Learn pipeline\n",
    "- Azure Machine Learning pipelines encapsulate steps that can be run as an experiment.\n",
    "- Azure DevOps pipelines: the build and configuration tasks required to deliver software.\n",
    "\n",
    "### Azure Machine Learning Pipeline\n",
    "\n",
    "In Azure Machine Learning, a pipeline is a workflow of machine learning tasks in which each task is implemented as a *step*.\n",
    "\n",
    "Steps can be arranged sequentially or in parallel, enabling you to build sophisticated flow logic to orchestrate machine learning operations. *Each step can be run on a specific compute target*, making it possible to combine different types of processing as required to achieve an overall goal.\n",
    "\n",
    "**A pipeline can be executed as a process by running the pipeline as an experiment. Each step in the pipeline runs on its allocated compute target as part of the overall experiment run.**\n",
    "\n",
    "You can **publish a pipeline as a REST endpoint**, enabling client applications to initiate a pipeline run. You can also **define a schedule** for a pipeline, and have it run automatically at periodic intervals.\n",
    "\n",
    "#### Types of step\n",
    "Common kinds of step in an Azure Machine Learning pipeline include:\n",
    "\n",
    "- **PythonScriptStep**: Runs a specified Python script.\n",
    "- **EstimatorStep**: Runs an estimator.\n",
    "- **DataTransferStepv**: Uses Azure Data Factory to copy data between data stores.\n",
    "- **DatabricksStep**: Runs a notebook, script, or compiled JAR on a databricks cluster.\n",
    "- **AdlaStep**: Runs a U-SQL job in Azure Data Lake Analytics.\n",
    "\n",
    "To create a pipeline, you must first define each step and then create a pipeline that includes the steps. The specific configuration of each step depends on the step type.\n",
    "#### Note: Each step has its own configuration done by a `pipeline.step` object. After each step object is created, a `Pipeline` object chains all steps together.\n",
    "\n",
    "```python\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config)\n",
    "\n",
    "# Step to run an estimator\n",
    "step2 = EstimatorStep(name = 'train model',\n",
    "                      estimator = sk_estimator,\n",
    "                      compute_target = 'aml-cluster')\n",
    "```\n",
    "\n",
    "After defining the steps, you can assign them to a pipeline, and run it as an experiment:\n",
    "\n",
    "```python\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Construct the pipeline\n",
    "train_pipeline = Pipeline(workspace = ws, steps = [step1,step2])\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
    "pipeline_run = experiment.submit(train_pipeline) # The configuration becomes a Pipeline object\n",
    "```\n",
    "\n",
    "### Pass data between pipeline steps\n",
    "\n",
    "The **PipelineData** object is a special kind of **DataReference** that:\n",
    "\n",
    "- References a location in a datastore.\n",
    "- Creates a **data dependency between pipeline steps**.\n",
    "\n",
    "To use a PipelineData object to pass data between steps, you must:\n",
    "\n",
    "1. Define a named PipelineData object that references a location in a datastore.\n",
    "2. Specify the PipelineData object as an input or output for the steps that use it.\n",
    "3. Pass the PipelineData object as a script parameter in steps that run scripts (and include code in those scripts to read or write data)\n",
    "\n",
    "#### Note: PipelineData object can also be used to pass trained models. There's another object called PipelineDataset for tabular data.\n",
    "\n",
    "```python\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Get a dataset for the initial data\n",
    "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
    "\n",
    "# 1. Define a PipelineData object to pass data between steps\n",
    "data_store = ws.get_default_datastore()\n",
    "prepped_data = PipelineData('prepped',  datastore=data_store)\n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config,\n",
    "                         # Specify dataset as initial input\n",
    "                         inputs=[raw_ds.as_named_input('raw_data')],\n",
    "                         # 2 & 3. Specify PipelineData as output and create dependency between steps\n",
    "                         outputs=[prepped_data],\n",
    "                         # Also pass as data reference to script\n",
    "                         arguments = ['--folder', prepped_data])\n",
    "\n",
    "# Step to run an estimator\n",
    "step2 = EstimatorStep(name = 'train model',\n",
    "                      estimator = sk_estimator,\n",
    "                      compute_target = 'aml-cluster',\n",
    "                      # 2 & 3. Specify PipelineData as input and create dependency between steps\n",
    "                      inputs=[prepped_data],\n",
    "                      # Pass as data reference to estimator script\n",
    "                      estimator_entry_script_arguments=['--folder', prepped_data])\n",
    "\n",
    "```\n",
    "\n",
    "Code in data_prep.py\n",
    "```python\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get input dataset as dataframe\n",
    "raw_df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "# code to prep data (in this case, just select specific columns)\n",
    "prepped_df = raw_df[['col1', 'col2', 'col3']]\n",
    "\n",
    "# Save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, 'prepped_data.csv')\n",
    "prepped_df.to_csv(output_path) # This would create a PipelineData object\n",
    "```\n",
    "\n",
    "### Reuse pipeline steps\n",
    "\n",
    "By default, the step output from a previous pipeline run is reused without rerunning the step provided the script, source directory, and other parameters for the step have not changed. **Step reuse can reduce the time it takes to run a pipeline, but it can lead to stale results when changes to downstream data sources have not been accounted for**.\n",
    "\n",
    "To control reuse for an individual step, you can set the `allow_reuse = False` in the step configuration\n",
    "\n",
    "When you have multiple steps, you can force all of them to run regardless of individual reuse configuration by setting the regenerate_outputs parameter when submitting the pipeline experiment:\n",
    "\n",
    "```python\n",
    "pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)\n",
    "```\n",
    "\n",
    "### Publish pipelines\n",
    "\n",
    "After you have created a pipeline, you can publish it to create a **REST endpoint** through which the pipeline can be run on demand.\n",
    "\n",
    "```python\n",
    "# Approach 1: publish method\n",
    "published_pipeline = pipeline.publish(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')\n",
    "\n",
    "# Approach 2: call the publish method on a successful run\n",
    "    # Get the most recent run of the pipeline\n",
    "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
    "run = list(pipeline_experiment.get_runs())[0]\n",
    "\n",
    "    # Publish the pipeline from the run\n",
    "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')\n",
    "```\n",
    "\n",
    "After the pipeline has been published, you can view it in Azure Machine Learning studio. You can also determine the URI of its endpoint like this:\n",
    "\n",
    "```python\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)\n",
    "```\n",
    "\n",
    "To use the endpoint, client applications need to make a REST call over HTTP. This request must be authenticated, so an authorization header is required. A real application would require a service principal with which to be authenticated, but to test this out, we'll use the authorization header from your current connection to your Azure workspace, which you can get using the following code:\n",
    "\n",
    "```python\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "```\n",
    "\n",
    "To initiate a published endpoint, you **make an HTTP request to its REST endpoint, passing an authorization header with a token for a service principal with permission to run the pipeline, and a JSON payload specifying the experiment name**. The pipeline is run asynchronously, so the response from a successful REST call includes the run ID. You can use this to track the run in Azure Machine Learning studio.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(run_id)\n",
    "```\n",
    "\n",
    "### Use pipeline parameters\n",
    "\n",
    "In the script:\n",
    "```python\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "reg_param = PipelineParameter(name='reg_rate', default_value=0.01) # create a PipelineParameter object\n",
    "\n",
    "...\n",
    "\n",
    "step2 = EstimatorStep(name = 'train model',\n",
    "                      estimator = sk_estimator,\n",
    "                      compute_target = 'aml-cluster',\n",
    "                      inputs=[prepped],\n",
    "                      estimator_entry_script_arguments=['--folder', prepped,\n",
    "                                                        '--reg', reg_param])\n",
    "```\n",
    "\n",
    "After you publish a parameterized pipeline, you can pass parameter values in the JSON payload for the REST interface:\n",
    "\n",
    "```python\n",
    "response = requests.post(rest_endpoint,\n",
    "                         headers=auth_header,\n",
    "                         json={\"ExperimentName\": \"run_training_pipeline\",\n",
    "                               \"ParameterAssignments\": {\"reg_rate\": 0.1}})\n",
    "```\n",
    "\n",
    "### Schedule pipelines\n",
    "```python\n",
    "# Scheduling a pipeline for periodic intervals: define a ScheduleRecurrence that determines the run frequency, and use it to create a Schedule.\n",
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "daily = ScheduleRecurrence(frequency='Day', interval=1)\n",
    "pipeline_schedule = Schedule.create(ws, name='Daily Training',\n",
    "                                        description='trains model every day',\n",
    "                                        pipeline_id=published_pipeline.id,\n",
    "                                        experiment_name='Training_Pipeline',\n",
    "                                        recurrence=daily # pass the ScheduleRecurrence object here\n",
    "                                   )\n",
    "\n",
    "# Triggering a pipeline run on data changes: create a Schedule that monitors a specified path on a datastore\n",
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                                    description='trains model on data change',\n",
    "                                    pipeline_id=published_pipeline_id,\n",
    "                                    experiment_name='Training_Pipeline',\n",
    "                                    datastore=training_datastore, # Pass the Datastore here\n",
    "                                    path_on_datastore='data/training')\n",
    "```\n",
    "\n",
    "### [Pattern for creating and using pipelines](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py#pattern-for-creating-and-using-pipelines)\n",
    "\n",
    "- **A Azure Machine learning Pipeline is associated with an <ins>Azure Machine Learning workspace</ins>.**\n",
    "- **A pipeline step is associated with a <ins>compute target</ins> within that workspace.**\n",
    "\n",
    "A common pattern for pipeline steps is:\n",
    "\n",
    "1. Specify workspace, compute, and storage\n",
    "2. Configure your input and output data using\n",
    "    - Dataset which makes available an existing Azure datastore\n",
    "    - PipelineDataset which encapsulates typed tabular data\n",
    "    - PipelineData which is used for intermediate file or directory data written by one step and intended to be consumed by another\n",
    "3. Define one or more pipeline steps\n",
    "4. Instantiate a pipeline using your workspace and steps\n",
    "5. Create an experiment to which you submit the pipeline\n",
    "6. Monitor the experiment results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
