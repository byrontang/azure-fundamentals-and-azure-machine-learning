{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Note - Building AI Solutions with Azure Machine Learning\n",
    "This notebook collects the notes taken through the course of **[Build AI solutions with Azure Machine Learning](https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/)** offered by Microsoft, with supplements from the **[documentation of Azure Machine Learning SDK for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Getting Started with Azure Machine Learning\n",
    "\n",
    "The Azure ML SDK for Python provides classes you can use to work with Azure ML in your Azure subscription.\n",
    "### azureml-core package\n",
    "**High level process:**\n",
    "1. **create a new <font color='blue'>*workspace*</font> or connect to an existing workspace** \n",
    "2. **create an Azure ML <font color='blue'>*experiment*</font> in workspace**\n",
    "3. **create a <font color='blue'>*run*</font> to run codes**\n",
    "\n",
    "#### Workspace   \n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "```\n",
    "- All experiments and associated resources are managed within you Azure ML workspace. You can connect to an existing workspace, or create a new one using the Azure ML SDK.\n",
    "\n",
    "```python\n",
    "# Load an existing workspace\n",
    "ws = Workspace.get(name=\"myworkspace\", subscription_id='<azure-subscription-id>', resource_group='myresourcegroup')\n",
    "\n",
    "# Create a new one\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='<azure-subscription-id>',\n",
    "                      resource_group='myresourcegroup',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2'\n",
    "                     )\n",
    "```\n",
    "\n",
    "- In most cases, you should store the workspace configuration in a JSON configuration file. This makes it easier to reconnect without needing to remember details like your Azure subscription ID.\n",
    "```python\n",
    "ws.write_config(path=\"./file-path\", file_name=\"ws_config.json\")\n",
    "```\n",
    "- You can download the JSON configuration file from the blade for your workspace in the Azure portal, but <ins>if you're using a Compute Instance within your workspace, the configuration file has already been downloaded to the root folder.</ins>\n",
    "- `.from_config()` finds and uses the configuration file from the root folder to connect to your workspace.\n",
    "```python\n",
    "ws_other_environment = Workspace.from_config(path=\"./file-path/ws_config.json\")\n",
    "```\n",
    "\n",
    "#### Experiment\n",
    "```python\n",
    "from azureml.core import Experiment\n",
    "```    \n",
    "- We use an Azure ML experiment to run Python code and record values extracted from data.\n",
    "- Create an Azure ML experiment in workspace\n",
    "```python\n",
    "experiment = Experiment(workspace=ws, name='test-experiment')\n",
    "```\n",
    "\n",
    "#### Run\n",
    "A run represent a single trial of an experiment.\n",
    "- There are two ways to create run:\n",
    "    1. `experiment.start_logging()`\n",
    "    2. azureml.core.Run \n",
    "```python\n",
    "from azureml.core import Run\n",
    "```\n",
    "        - Create a separate script from experiment, store it in a folder along with any other files it needs, and then use Azure ML to run the experiment based on the script in the folder.\n",
    "        - `Run.get_context()` method to retrieve the experiment run context when the script is run\n",
    "\n",
    "<ins>**After a run object is created, use various `.log*()` methods to log the outputs.**</ins>\n",
    "\n",
    "#### Configuration\n",
    "```python\n",
    "from azureml.core import Experiment, RunConfiguration, ScriptRunConfig\n",
    "```\n",
    "- Please refer to [previous note] for better understanding\n",
    "\n",
    "## MLflow\n",
    "\n",
    "**MLflow** is an open source platform for managing machine learning processes. It's **commonly (but not exclusively) used in Databricks environments** to coordinate experiments and track metrics. In Azure Machine Learning experiments, you can use MLflow to track metrics instead of the native log functionality if you desire.\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "```\n",
    "- Refer to the notebook codes in official Git-Hub fore more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Training Models\n",
    "### Key Concepts:\n",
    "- Create a training script and log key metrics of modeling performance\n",
    "- Use an Estimator to run the script as experiment\n",
    "```python \n",
    "from azureml.train.estimator import Estimator\n",
    "```\n",
    "- View logged metrics\n",
    "```python\n",
    "from azureml.widgets import RunDetails\n",
    "```\n",
    "- Register the trained model to the workspace\n",
    "```python\n",
    "from azureml.core import Model\n",
    "```\n",
    "- Create a parameterized training script: Adding parameters to your script enables you to repeat the same training experiment with different settings\n",
    "```python\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "```\n",
    "- Use a framework-specific Estimator\n",
    "```python\n",
    "from azureml.train.sklearn import SKLearn\n",
    "```\n",
    "- Register a new version of the model\n",
    "    - The version of the same model will be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: Revisit how to interprete ROC\n",
    "- Y axis calculates True Positive Rate – the base is True (Ex: 80 True instances)\n",
    "- X axis calculates False Positive Rate – the base is False (Ex: 20 False instances)\n",
    "    - If we select True by randomly, the probability of selecting a true or false instance is 0.8 and 0.2. Therefore, TPR and FPR will increase at around the same pace.\n",
    "    - However, if we build a good predictive model, the probability of selecting a true instance should increase, skewing the curve to the top-left. ***The better the capability of the model to predict true positive, the higher the AUC.***\n",
    "\n",
    "#### Don’t confuse the concept of AUC and Accuracy.\n",
    "- AUC shows **the capability of a model to predict true positives**, and each axis has different base.\n",
    "- The base of accuracy includes both true and false instances. It doesn’t take into account the capability of predicting true positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Work with Data in Azure Machine Learning\n",
    "### Datastores\n",
    "- In Azure Machine Learning, ***datastores*** are abstractions for cloud data sources / storage locations. \n",
    "    -\tUpload data\n",
    "        - We can upload data to a datastore. When you uploaded the files using `.upload_files()`, note that the code returned a *data reference*.\n",
    "    - Configure data reference\n",
    "        - We can configures the data reference for ***download*** - in other words, it can be used to download the contents of the folder to the compute context where the data reference is being used. \n",
    "        - When working with remote compute, you can also configure a data reference to ***mount*** the datastore location and read data directly from the data source. \n",
    "    -\tTrain a model from a datastore\n",
    "        - The **data reference** can then be passed to a training script as a parameter.\n",
    "\n",
    "### Datasets\n",
    "- ***Datasets*** are versioned packaged data objects that can be easily consumed in experiments and pipelines.\n",
    "    - Create datasets\n",
    "        - While you can **read data directly from datastores**, Azure Machine Learning provides a further abstraction for data in the form of *datasets*. (As apposed to data reference)\n",
    "        - A dataset is a versioned reference to a specific set of data that you may want to use in an experiment. Datasets can be *tabular* or *file-based*.\n",
    "        - It's easy to convert a tabular dataset to a Pandas dataframe, enabling you to work with the data using common python techniques.\n",
    "    - Register datasets\n",
    "    - Train a model from a dataset\n",
    "        - Pass the dataset as an input in the estimator being used to run the script \n",
    "            - In the script: `run.input_datasets['diabetes'].to_pandas_dataframe()`\n",
    "                - **Note: This line grabs the input dataset named ‘diabetes’ from the run submitted.**\n",
    "            - `diabetes_ds = ws.datasets.get(\"diabetes dataset\")`\n",
    "                - **Note: Before configure a run, we need to set up an dataset object using the dataset in workspace**\n",
    "            - In SKLearn(): `inputs=[diabetes_ds.as_named_input('diabetes')]`\n",
    "                - **Note: The dataset object can then be passed to the inputs argument when configuring Estimator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Work with Compute in Azure machine Learning\n",
    "The runtime context for each experiment run consists of two elements:\n",
    "1. The *environment* for the script, which includes all packages used in the script.\n",
    "2. The *compute target* on which the environment will be deployed and the script run. This could be the local workstation from which the experiment run is initiated, or a remote compute target such as a training cluster that is provisioned on-demand.\n",
    "    - In Azure Machine Learning, *Compute Targets* are **physical or virtual computers on which experiments are run.\n",
    "\n",
    "When you run a Python script as an experiment in Azure Machine Learning, a Conda environment is created to define the execution context for the script. Azure Machine Learning provides a default environment that includes many common packages; including the **azureml-defaults** package that contains the libraries necessary for working with an experiment run, as well as popular packages like **pandas** and **numpy**.\n",
    "\n",
    "\n",
    "You can also define your own environment and add packages by using **conda** or **pip**, to ensure your experiment has access to all the libraries it requires.\n",
    "```python\n",
    "estimator = Estimator (source_directory=experiment_folder,\n",
    "                       inputs=[diabetes_ds.as_named_input('diabetes')],\n",
    "                       script_params=script_params,\n",
    "                       compute_target = 'local', # OR compute_target = cluster_name # Run on the remote compute target\n",
    "                       environment_definition = diabetes_env, # environment\n",
    "                       entry_script='diabetes_training.py')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 Orchestra machine learning with pipelines\n",
    "\n",
    "### Definition\n",
    "The term pipeline is used extensively in machine learning, often with different meanings.\n",
    "- Scikit-Learn pipeline\n",
    "- Azure Machine Learning pipelines: a workflow of machine learning tasks in which each task is implemented as a *step*. **Check bookmark: Introduction to Pipelines**\n",
    "- Azure DevOps pipelines: the build and configuration tasks required to deliver software.\n",
    "\n",
    "### Key objects for building a pipeline\n",
    "- Steps\n",
    "```python\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "```\n",
    "    - A pipeline is like an experiment, and each step is like a part of the experiment. \n",
    "- PipelineData\n",
    "```python\n",
    "from azureml.pipeline.core import PipelineData\n",
    "```\n",
    "    - To use a PipelineData object to pass data between steps, you must:\n",
    "        - Define a named PipelineData object that references **a location** in a datastore. **(BIG NOTE: The PipelineData is only a reference to a location. It is NOT a dataset.)**\n",
    "        - Specify the PipelineData object as an input or output for the steps that use it.\n",
    "        - Pass the PipelineData object as **a script parameter** in steps that run scripts (and include code in those scripts to read or write data)**(Note: Specify the location parameter in script.)**\n",
    "- Pipeline\n",
    "```python\n",
    "from azureml.pipeline.core import Pipeline\n",
    "```\n",
    "\n",
    "### [Pattern for creating and using pipelines](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py#pattern-for-creating-and-using-pipelines)\n",
    "\n",
    "- **A Azure Machine learning Pipeline is associated with an <ins>Azure Machine Learning workspace</ins>.**\n",
    "- **A pipeline step is associated with a <ins>compute target</ins> within that workspace.**\n",
    "\n",
    "A common pattern for pipeline steps is:\n",
    "\n",
    "1. Specify workspace, compute, and storage\n",
    "2. Configure your input and output data using\n",
    "    - Dataset which makes available an existing Azure datastore\n",
    "    - PipelineDataset which encapsulates typed tabular data\n",
    "    - PipelineData which is used for intermediate file or directory data written by one step and intended to be consumed by another\n",
    "3. Define one or more pipeline steps\n",
    "4. Instantiate a pipeline using your workspace and steps\n",
    "5. Create an experiment to which you submit the pipeline\n",
    "6. Monitor the experiment results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Deploy real-time machine learning services with Azure Machine Learning\n",
    "In machine learning, *inferencing* refers to the use of a trained model to predict labels for new data on which the model has not been trained. In Azure Machine learning, you can create **real-time inferencing solutions by deploying a model as a service**, hosted in a **containerized platform** such as **Azure Kubernetes Services (AKS)**.\n",
    "\n",
    "**Notes:** \n",
    "- **We can also deploy the model on Azure Container Instances (ACI) Web Service or local Docker-based service during development and testing.**\n",
    "- **ACI web service is best for small scale testing and quick deployments, and AKS is for deloyments as a production-scale web service.**\n",
    "\n",
    "To deploy a model as a real-time inferencing service, you must perform the following tasks:\n",
    "1.\tRegister a trained model\n",
    "2.\tDefine an inference configuration\n",
    "    1.\tCreate an **entry script**: The entry script receives data submitted to a deployed web service and passes it to the model. It then takes the response returned by the model and returns that to the client. *The script is specific to your model.* It must understand the data that the model expects and returns.\n",
    "        1.\t`init()`: Called when the service is initialized - Typically, this function loads the model into a global object. This function is run only once, when the Docker container for your web service is started.\n",
    "        2.\t`run(inpute_data)`: Called with new data is submitted to the service - This function uses the model to predict a value based on the input data. Inputs and outputs of the run typically use JSON for serialization and deserialization. You can also work with raw binary data. You can transform the data before sending it to the model or before returning it to the client.\n",
    "\n",
    "            ```python\n",
    "            [To include entry script codes]\n",
    "            ```\n",
    "        \n",
    "    2.\tCreate an environment\n",
    "    3.\tCombine the script and environment in an InferenceConfig\n",
    "\n",
    "    ```python\n",
    "    from azureml.core.model import InferenceConfig\n",
    "\n",
    "    classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                                  source_directory = 'service_files',\n",
    "                                                  entry_script=\"score.py\",\n",
    "                                                  conda_file=\"env.yml\")\n",
    "    ```\n",
    "\n",
    "3.\tDefine a deployment configuration on the chosen compute target\n",
    "    - AksCompute\n",
    "    ```python\n",
    "    from azureml.core.compute import ComputeTarget, AksCompute\n",
    "    from azureml.core.webservice import AksWebservice\n",
    "    ```\n",
    "\n",
    "    - ACI deployment\n",
    "    ```python\n",
    "    from azureml.core.webservice import AciWebservice\n",
    "    ```\n",
    "    \n",
    "    - local Docker-based service\n",
    "    ```python\n",
    "    from azureml.core.webservice import LocalWebservice\n",
    "    ```\n",
    "4.\tDeploy the model\n",
    "```python\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model], #1. Model registered\n",
    "                       inference_config = classifier_inference_config, # 2. Inference Configuration\n",
    "                       deployment_config = classifier_deploy_config, # 3. deployment configuration\n",
    "                       deployment_target = production_cluster) # (Optional) 3. deployment configuration\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)\n",
    "```\n",
    "\n",
    "To delete a deployed web service, use `service.delete()`. To delete a registered model, use `model.delete()`.\n",
    "\n",
    "#### [Additional Topic: Create an endpoint](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service#create-an-endpoint)\n",
    "\n",
    "**To create an endpoint, use `AksEndpoint.deploy_configuration` instead of `AksWebservice.deploy_configuration()`.**\n",
    "\n",
    "```python\n",
    "import azureml.core,\n",
    "from azureml.core.webservice import AksEndpoint\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "# select a created compute\n",
    "compute = ComputeTarget(ws, 'myaks')\n",
    "namespace_name= endpointnamespace\n",
    "# define the endpoint and version name\n",
    "endpoint_name = \"mynewendpoint\"\n",
    "version_name= \"versiona\"\n",
    "# create the deployment config and define the scoring traffic percentile for the first deployment\n",
    "endpoint_deployment_config = AksEndpoint.deploy_configuration(cpu_cores = 0.1, memory_gb = 0.2,\n",
    "                                                              enable_app_insights = True,\n",
    "                                                              tags = {'sckitlearn':'demo'},\n",
    "                                                              description = \"testing versions\",\n",
    "                                                              version_name = version_name,\n",
    "                                                              traffic_percentile = 20)\n",
    " # deploy the model and endpoint\n",
    " endpoint = Model.deploy(ws, endpoint_name, [model], inference_config, endpoint_deployment_config, compute)\n",
    " # Wait for he process to complete\n",
    " endpoint.wait_for_deployment(True)\n",
    "```\n",
    "\n",
    "To *consume* a deployed real-time service (or model or endpoint), we’ll need the following: **(Note: Recall the consume tab in AML Studio.)**\n",
    "-\tHTTP Post/ Url **(Note: Recall the step to copy the REST url on AML studio and paste it in the script.)**\n",
    "-\tKey **(Note: Recall the step to copy the primary key on AML studio and paste it in the script.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 Deploy batch inference pipelines with Azure Machine Learning\n",
    "The steps are not very consistent between lectures and lab codes. Refer to the lab codes when there’s inconsistency.\n",
    "1.\tRegister a model\n",
    "2.\tCreate a scoring script and define a run context that includes the dependencies required by the script\n",
    "3.\tCreate a pipeline with **ParallelRunStep** (As the chapter name suggests, we’re going to create a step in pipeline)\n",
    "```python\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "# Define the parallel run step step configuration\n",
    "# Create the parallel run step\n",
    "```\n",
    "4.\tRun the pipeline and retrieve the step output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08 Tune hyperparameters with Azure Machine Learning\n",
    "For a discrete parameter, use a **choice** from a list of explicit values. Example: `'--batch_size': choice(16, 32, 64)`\n",
    "### Type of sampling:\n",
    "- Grid sampling\n",
    "- Random sampling\n",
    "- Bayesian sampling\n",
    "\n",
    "### Early termination\n",
    "- Bandit policy: stop a run if the target performance metric underperforms the best run so far by a specified margin\n",
    "```python\n",
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "```\n",
    "- Median stopping policy: abandons runs where the target performance metric is worse than the median of the running averages for all runs\n",
    "```python\n",
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "```\n",
    "- Truncation selection policy: cancels the lowest performing X% of runs at each evaluation interval based on the truncation_percentage value you specify for X\n",
    "```python\n",
    "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
    "```\n",
    "\n",
    "### Running a hyperparameter tuning experiment\n",
    "- Create a training script that\n",
    "    - Includes an argument for each hyperparameter you want to vary (covered in previous lab)\n",
    "    - Log the target performance metric (covered in previous lab)\n",
    "- Configure and run hyperdrive experiment\n",
    "```python\n",
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "```\n",
    "- Monitor and review hyperdrive runs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 Automate machine learning model selection with Azure Machine Learning\n",
    "Automated Machine Learning is one of the two big features, Automated ML and Designer, in AML studio. You can use the visual interface in Azure Machine Learning studio or the SDK to leverage this capability. The SDK gives you greater control over the settings for the automated machine learning experiment, but the visual interface is easier to use.\n",
    "\n",
    "Configure an Automated Machine Learning experiment: \n",
    "```python\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Explain machine learning models with Azure Machine Learning\n",
    "\n",
    "### Type of Feature Importance\n",
    "- **Global feature importance** quantifies the relative importance of each feature in the test dataset as a whole.\n",
    "- **Local feature importance** measures the influence of each feature value for a specific individual prediction.\n",
    "\n",
    "### Explainers\n",
    "Using explainers – install the **azureml-interpret** package\n",
    "- MimicExplainer – An explainer that creates a *global surrogate model* that approximates your trained model and can be used to generate explanations.\n",
    "```python\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import DecisionTreeExplainableModel # Requires most arguments\n",
    "```\n",
    "- TabularExplainer – An explainer that acts as a wrapper around various SHAP explainer algorithms, automatically choosing the one that is most appropriate for your model architecture.\n",
    "```python\n",
    "from interpret.ext.blackbox import TabularExplainer # Does not require explainable_model\n",
    "```\n",
    "- PFIExplainer – a *Permutation Feature Importance* explainer that analyzes feature importance by shuffling feature values and measuring the impact on prediction performance.\n",
    "```python\n",
    "from interpret.ext.blackbox import PFIExplainer #Does not require explainable_model and initialization_example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Detect and mitigate unfairness in models with Azure Machine Learning\n",
    "\n",
    "### Disparity\n",
    "[To add more notes]\n",
    "\n",
    "\n",
    "A model with lower disparity in predictive performance between sensitive feature groups might be favorable then the model with higher disparity and overall accuracy.\n",
    "\n",
    "#### Side Note – under what situations we might choose a model with lower accuracy/AUC over a higher one?\n",
    "- Time required for training\n",
    "- Interpretability\n",
    "- Lower disparity between sensitive feature groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Monitor a Model\n",
    "To capture telemetry data for Application insights, you can write any values to the standard output log in the scoring script for your service by using a print statement\n",
    "\n",
    "Summarize the whole workflow from building, deploying, consuming, to monitoring a model.\n",
    "- Refer to Jupyter Notebook for the complete codes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
