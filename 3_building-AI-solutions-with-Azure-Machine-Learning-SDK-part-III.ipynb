{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Note - Building AI Solutions with Azure Machine Learning\n",
    "This notebook collects the notes taken through the course of **[Build AI solutions with Azure Machine Learning](https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/)** offered by Microsoft, with supplements from the **[documentation of Azure Machine Learning SDK for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py)**.\n",
    "\n",
    "This notebook contains Labs 08 - 13 of the learning course, which correspond to \"Optimize and Manage Models\" section in the exam guideline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08 Tune hyperparameters with Azure Machine Learning\n",
    "\n",
    "Hyperparameter tuning is accomplished by training the multiple models, using the same algorithm and training data but different hyperparameter values. The resulting model from each training run is then evaluated to determine the performance metric for which you want to optimize (for example, accuracy), and the best-performing model is selected.\n",
    "\n",
    "In Azure Machine Learning, you achieve this through an experiment that consists of a hyperdrive run, which initiates **a child run** for each hyperparameter combination to be tested. Each child run uses a training script with parameterized hyperparameter values to train a model, and logs the target performance metric achieved by the trained model.\n",
    "\n",
    "### Defining a search space\n",
    "\n",
    "For discrete distributions:\n",
    "- qnormal\n",
    "- quniform\n",
    "- qlognormal\n",
    "- qloguniform\n",
    "\n",
    "For continous distributions:\n",
    "- normal\n",
    "- uniform\n",
    "- lognormal\n",
    "- loguniform\n",
    "\n",
    "\n",
    "```python\n",
    "from azureml.train.hyperdrive import choice, normal\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }\n",
    "```\n",
    "\n",
    "For a discrete parameter, use a **choice** from a list of explicit values. Example: `'--batch_size': choice(16, 32, 64)`\n",
    "\n",
    "### Configuring sampling\n",
    "#### Grid sampling\n",
    "\n",
    "Grid sampling can only be employed when all hyperparameters are discrete, and is used to try every possible combination of parameters in the search space.\n",
    "\n",
    "```python\n",
    "from azureml.train.hyperdrive import GridParameterSampling, choice\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': choice(0.01, 0.1, 1.0)\n",
    "              }\n",
    "\n",
    "param_sampling = GridParameterSampling(param_space)\n",
    "```\n",
    "\n",
    "#### Random sampling\n",
    "\n",
    "Random sampling is used to randomly select a value for each hyperparameter, which can be a mix of discrete and continuous values\n",
    "\n",
    "```python\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, normal\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': normal(10, 3)\n",
    "              }\n",
    "\n",
    "param_sampling = RandomParameterSampling(param_space)\n",
    "```\n",
    "\n",
    "#### Bayesian sampling\n",
    "\n",
    "Bayesian sampling chooses hyperparameter values based on the Bayesian optimization algorithm, which tries to select parameter combinations that will result in improved performance from the previous selection.\n",
    "\n",
    "```python\n",
    "from azureml.train.hyperdrive import BayesianParameterSampling, choice, uniform\n",
    "\n",
    "param_space = {\n",
    "                 '--batch_size': choice(16, 32, 64),\n",
    "                 '--learning_rate': uniform(0.5, 0.1)\n",
    "              }\n",
    "\n",
    "param_sampling = BayesianParameterSampling(param_space)\n",
    "```\n",
    "\n",
    "You can only use Bayesian sampling with **choice, uniform, and quniform** parameter expressions, and you can't combine it with an early-termination policy.\n",
    "\n",
    "\n",
    "### Configuring early termination\n",
    "\n",
    "To help prevent wasting time, you can **set an early termination policy** that abandons runs that are unlikely to produce a better result than previously completed runs. The policy is evaluated at an ***evaluation_interval*** you specify, based on each time the target performance metric is logged. You can also set a ***delay_evaluation*** parameter to avoid evaluating the policy until a minimum number of iterations have been completed.\n",
    "\n",
    "\n",
    "#### Bandit policy\n",
    "\n",
    "You can use a bandit policy to stop a run if the target performance metric underperforms the best run so far by a specified margin.\n",
    "\n",
    "```python\n",
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_amount = 0.2,\n",
    "                                        evaluation_interval=1,\n",
    "                                        delay_evaluation=5)\n",
    "```\n",
    "This example applies the policy for every iteration after the first five, and abandons runs where the reported target metric is 0.2 or more worse than the best performing run **after the same number of intervals**.\n",
    "\n",
    "You can also apply a bandit policy using a slack factor, which compares the performance metric as a ratio rather than an absolute value.\n",
    "\n",
    "#### Median stopping policy\n",
    "\n",
    "A median stopping policy abandons runs where the target performance metric is worse than the median of the running averages for all runs.\n",
    "\n",
    "```python\n",
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1,\n",
    "                                                delay_evaluation=5)\n",
    "```\n",
    "\n",
    "#### Truncation selection policy\n",
    "\n",
    "A truncation selection policy cancels the lowest performing X% of runs at each evaluation interval based on the truncation_percentage value you specify for X.\n",
    "\n",
    "```python\n",
    "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
    "\n",
    "early_termination_policy = TruncationSelectionPolicy(truncation_percentage=10,\n",
    "                                                     evaluation_interval=1,\n",
    "                                                     delay_evaluation=5)\n",
    "```\n",
    "\n",
    "### Running a hyperparameter tuning experiment\n",
    "\n",
    "In Azure Machine Learning, you can tune hyperparameters by running a ***hyperdrive experiment***.\n",
    "\n",
    "To run a hyperdrive experiment, you need to create a training script just the way you would do for any other training experiment, except that your script must:\n",
    "\n",
    "- Include an argument for each hyperparameter you want to vary.\n",
    "- Log the target performance metric. This enables the hyperdrive run to evaluate the performance of the child runs it initiates, and identify the one that produces the best performing model.\n",
    "\n",
    "#### Note: The above two requirements should already be met in the script based on previous lectures\n",
    "\n",
    "To prepare the hyperdrive experiment, you must use a **HyperDriveConfig** object to configure the experiment run, as shown in the following example code:\n",
    "\n",
    "```python\n",
    "# Configuring and running a hyperdrive experiment\n",
    "from azureml.core import Experiment\n",
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal\n",
    "\n",
    "# Assumes ws, sklearn_estimator and param_sampling are already defined\n",
    "\n",
    "hyperdrive = HyperDriveConfig(estimator=sklearn_estimator,\n",
    "                              hyperparameter_sampling=param_sampling, # Sampleing method\n",
    "                              policy=None,\n",
    "                              primary_metric_name='Accuracy',\n",
    "                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                              max_total_runs=6,\n",
    "                              max_concurrent_runs=4)\n",
    "\n",
    "experiment = Experiment(workspace = ws, name = 'hyperdrive_training')\n",
    "hyperdrive_run = experiment.submit(config=hyperdrive)\n",
    "```\n",
    "\n",
    "You can monitor hyperdrive experiments in Azure Machine Learning studio, or by using the Jupyter Notebooks **RunDetails** widget.\n",
    "\n",
    "The experiment will initiate a child run for each hyperparameter combination to be tried, and you can retrieve the logged metrics these runs using the following code:\n",
    "\n",
    "```python\n",
    "for child_run in run.get_children():\n",
    "    print(child_run.id, child_run.get_metrics())\n",
    "    \n",
    "# list all runs in descending order of performance like this\n",
    "for child_run in hyperdrive_.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)\n",
    "    \n",
    "# retrieve the best performing run\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 Automate machine learning model selection with Azure Machine Learning\n",
    "\n",
    "Azure Machine Learning includes support for automated machine learning through **a visual interface in Azure Machine Learning studio for *Enterprise* edition workspaces only**. You can **use the Azure Machine Learning SDK to run automated machine learning experiments in either *Basic* or *Enterprise* edition workspaces**.\n",
    "\n",
    "Automated Machine Learning is one of the two big features, Automated ML and Designer, in AML studio. You can use the visual interface in Azure Machine Learning studio or the SDK to leverage this capability. The SDK gives you greater control over the settings for the automated machine learning experiment, but the visual interface is easier to use.\n",
    "\n",
    "### Models\n",
    "You can use automated machine learning in Azure Machine Learning to train models for the following types of machine learning task:\n",
    "\n",
    "- Classification\n",
    "- Regression\n",
    "- Time Series Forecasting\n",
    "\n",
    "### Data Preprocessing\n",
    "As well as trying a selection of algorithms, automated machine learning can apply preprocessing transformations to your data; improving the performance of the model.\n",
    "\n",
    "Automated machine learning applies **scaling and normalization** to numeric data automatically, helping prevent any large-scale features from dominating training. *During an automated machine learning experiment, multiple scaling or normalization techniques will be applied.*\n",
    "\n",
    "You can choose to have automated machine learning apply **optional** preprocessing transformations such as:\n",
    "\n",
    "- Missing value imputation to eliminate nulls in the training dataset.\n",
    "- Categorical encoding to convert categorical features to numeric indicators.\n",
    "- Dropping high-cardinality features, such as record IDs.\n",
    "- Feature engineering (for example, deriving individual date parts from DateTime features)\n",
    "- Others...\n",
    "\n",
    "### Data\n",
    "When using the SDK to run an automated machine learning experiment, you can submit the data in the following ways:\n",
    "\n",
    "- Specify a dataset or dataframe of training data that includes features and the label to be predicted.\n",
    "- Optionally, specify a second validation data dataset or dataframe that will be used to validate the trained model. if this is not provided, Azure Machine Learning will apply cross-validation using the training data.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "- Specify a dataset, dataframe, or numpy array of X values containing the training features, with a corresponding y array of label values.\n",
    "- Optionally, specify X_valid and y_valid datasets, dataframes, or numpy arrays of X_valid values to be used for validation.\n",
    "\n",
    "### Codes\n",
    "\n",
    "```python\n",
    "# Configuring an Automated Machine Learning Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_run_config = RunConfiguration(framework='python')\n",
    "automl_config = AutoMLConfig(name='Automated ML Experiment',\n",
    "                             task='classification',\n",
    "                             primary_metric = 'AUC_weighted',\n",
    "                             compute_target=aml_compute,\n",
    "                             training_data = train_dataset,\n",
    "                             validation_data = test_dataset,\n",
    "                             label_column_name='Label',\n",
    "                             featurization='auto',\n",
    "                             iterations=12,\n",
    "                             max_concurrent_iterations=4)\n",
    "\n",
    "    # Retrieve the list of metrics available for a particular task type\n",
    "# from azureml.train.automl.utilities import get_primary_metrics\n",
    "# get_primary_metrics('classification')\n",
    "\n",
    "# Submitting an Automated Machine Learning Experiment\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "automl_experiment = Experiment(ws, 'automl_experiment')\n",
    "automl_run = automl_experiment.submit(automl_config)\n",
    "\n",
    "# Retrieving the Best Run and its Model\n",
    "best_run, fitted_model = automl_run.get_output()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "for metric_name in best_run_metrics:\n",
    "    metric = best_run_metrics[metric_name]\n",
    "    print(metric_name, metric)\n",
    "```\n",
    "\n",
    "Automated machine learning uses scikit-learn pipelines to encapsulate preprocessing steps with the model. You can view the steps in the fitted model you obtained from the best run using the code above like this:\n",
    "\n",
    "```python\n",
    "for step_ in fitted_model.named_steps:\n",
    "    print(step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Explain machine learning models with Azure Machine Learning\n",
    "\n",
    "As machine learning becomes increasingly integral to decisions that affect health, safety, economic wellbeing, and other aspects of people's lives, it's important to be able to understand **how models make predictions**; and to be able to **explain the rationale for machine learning based decisions**.\n",
    "\n",
    "### Type of Feature Importance\n",
    "\n",
    "Model explainers use statistical techniques to calculate feature importance. This enables you to quantify the relative influence each feature in the training dataset has on label prediction. Explainers work by evaluating a test data set of feature cases and the labels the model predicts for them.\n",
    "\n",
    "- **Global feature importance** quantifies the relative importance of each feature in the test dataset as a whole. It provides a general comparison of the extent to which each feature in the dataset influences prediction.\n",
    "- **Local feature importance** measures the influence of each feature value for a specific individual prediction.\n",
    "\n",
    "There could be multiple reasons why local importance for an individual prediction varies from global importance for the overall dataset; for example, Sam might have a lower income than average, but the loan amount in this case might be unusually small.\n",
    "\n",
    "For a multi-class classification model, a local importance values for each possible class is calculated for every feature, with the total across all classes always being 0.\n",
    "\n",
    "For a regression model, there are no classes so the local importance values simply indicate the level of influence each feature has on the predicted scalar label.\n",
    "\n",
    "### Explainers\n",
    "\n",
    "You can use the Azure Machine Learning SDK to create explainers for models, even if they were not trained using an Azure Machine Learning experiment.\n",
    "\n",
    "To interpret a local model, you must install the **azureml-interpret** package and use it to create an explainer. There are multiple types of explainer.\n",
    "\n",
    "#### MimicExplainer \n",
    "\n",
    "An explainer that creates a *global surrogate model* that approximates your trained model and can be used to generate explanations. This explainable model must have the same kind of architecture as your trained model (for example, linear or tree-based).\n",
    "\n",
    "```python\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
    "\n",
    "mim_explainer = MimicExplainer(model=loan_model,\n",
    "                             initialization_examples=X_test,\n",
    "                             explainable_model = DecisionTreeExplainableModel,\n",
    "                             features=['loan_amount','income','age','marital_status'], \n",
    "                             classes=['reject', 'approve'])\n",
    "```\n",
    "\n",
    "#### TabularExplainer \n",
    "\n",
    "An explainer that acts as a wrapper around various SHAP explainer algorithms, automatically choosing the one that is most appropriate for your model architecture.\n",
    "\n",
    "```python\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "tab_explainer = TabularExplainer(model=loan_model,\n",
    "                             initialization_examples=X_test,\n",
    "                             features=['loan_amount','income','age','marital_status'],\n",
    "                             classes=['reject', 'approve'])\n",
    "```\n",
    "#### PFIExplainer \n",
    "\n",
    "A *Permutation Feature Importance* explainer that analyzes feature importance by shuffling feature values and measuring the impact on prediction performance.\n",
    "\n",
    "```python\n",
    "from interpret.ext.blackbox import PFIExplainer\n",
    "\n",
    "pfi_explainer = PFIExplainer(model = loan_model,\n",
    "                             features=['loan_amount','income','age','marital_status'],\n",
    "                             classes=['reject', 'approve'])\n",
    "```\n",
    "\n",
    "### Explaining global feature importance\n",
    "\n",
    "To retrieve global importance values for the features in your mode, you call the **explain_global()** method of your explainer to get a global explanation, and then use the **get_feature_importance_dict()** method to get a dictionary of the feature importance values.\n",
    "\n",
    "```python\n",
    "# MimicExplainer\n",
    "global_mim_explanation = mim_explainer.explain_global(X_train)\n",
    "global_mim_feature_importance = global_mim_explanation.get_feature_importance_dict()\n",
    "\n",
    "\n",
    "# TabularExplainer\n",
    "global_tab_explanation = tab_explainer.explain_global(X_train)\n",
    "global_tab_feature_importance = global_tab_explanation.get_feature_importance_dict()\n",
    "\n",
    "\n",
    "# PFIExplainer\n",
    "global_pfi_explanation = pfi_explainer.explain_global(X_train, y_train) # requires the actual lables\n",
    "global_pfi_feature_importance = global_pfi_explanation.get_feature_importance_dict()\n",
    "```\n",
    "\n",
    "### Explaining local feature importance\n",
    "\n",
    "```python\n",
    "# MimicExplainer\n",
    "local_mim_explanation = mim_explainer.explain_local(X_test[0:5])\n",
    "local_mim_features = local_mim_explanation.get_ranked_local_names()\n",
    "local_mim_importance = local_mim_explanation.get_ranked_local_values()\n",
    "\n",
    "\n",
    "# TabularExplainer\n",
    "local_tab_explanation = tab_explainer.explain_local(X_test[0:5])\n",
    "local_tab_features = local_tab_explanation.get_ranked_local_names()\n",
    "local_tab_importance = local_tab_explanation.get_ranked_local_values()\n",
    "\n",
    "# The PFIExplainer doesn't support local feature importance explanations.\n",
    "```\n",
    "\n",
    "### Creating explanations\n",
    "\n",
    "When you use an estimator or a script to train a model in an Azure Machine Learning experiment, you can create an explainer and upload the explanation it generates to the run for later analysis.\n",
    "\n",
    "To **create an explanation in the experiment script**, you'll need to ensure that the **azureml-interpret** and **azureml-contrib-interpret** packages are installed in the run environment. Then you can use these to create an explanation from your trained model and upload it to the run outputs.\n",
    "\n",
    "```python\n",
    "# Import Azure ML run library\n",
    "from azureml.core.run import Run\n",
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "# other imports as required\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# code to train model goes here\n",
    "\n",
    "# Get explanation\n",
    "explainer = TabularExplainer(model, X_train, features=features, classes=labels)\n",
    "explanation = explainer.explain_global(X_test)\n",
    "\n",
    "# Get an Explanation Client and upload the explanation\n",
    "explain_client = ExplanationClient.from_run(run)\n",
    "explain_client.upload_model_explanation(explanation, comment='Tabular Explanation')\n",
    "\n",
    "# Complete the run\n",
    "run.complete()\n",
    "```\n",
    "\n",
    "You can view the explanation you created for your model in the **Explanations** tab for the run in Azure Machine learning studio.\n",
    "\n",
    "You can also use the **ExplanationClient** object to download the explanation in Python.\n",
    "\n",
    "```python\n",
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "\n",
    "client = ExplanationClient.from_run_id(workspace=ws,\n",
    "                                       experiment_name=experiment.experiment_name, \n",
    "                                       run_id=run.id)\n",
    "explanation = client.download_model_explanation()\n",
    "feature_importances = explanation.get_feature_importance_dict()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Detect and mitigate unfairness in models with Azure Machine Learning\n",
    "\n",
    "### Disparity\n",
    "[To add more notes]\n",
    "\n",
    "To evaluate the fairness of a model, you can apply the same predictive performance metric to subsets of the data, based on the sensitive features on which your population is grouped, and measure the disparity in those metrics across the subgroups.\n",
    "\n",
    "For example, suppose the loan approval model exhibits an overall recall metric of 0.67 - in other words, it correctly identifies 67% of cases where the applicant repaid the loan. The question is whether or not the model provides a similar rate of correct predictions for different age groups.\n",
    "\n",
    "Let's say that we find that the recall for validation cases where the applicant is 25 or younger is 0.50, and recall for cases where the applicant is over 25 is 0.83. In other words, the model correctly identified 50% of the people in the 25 or younger age group who successfully repaid a loan (and therefore misclassified 50% of them as loan defaulters), but found 83% of loan repayers in the older age group (misclassifying only 17% of them). The disparity in prediction performance between the groups is 33%, with the model predicting significantly more false negatives for the younger age group.\n",
    "\n",
    "A model with lower disparity in predictive performance between sensitive feature groups might be favorable then the model with higher disparity and overall accuracy.\n",
    "\n",
    "#### Side Note – under what situations we might choose a model with lower accuracy/AUC over a higher one?\n",
    "- Time required for training\n",
    "- Interpretability\n",
    "- Lower disparity between sensitive feature groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Monitor models with Azure Machine Learning\n",
    "\n",
    "After a machine learning model has been deployed into production, it's important to understand how it is being used by capturing and viewing **telemetry**.\n",
    "\n",
    "**Application Insights** is an application performance management service in Microsoft Azure that enables the capture, storage, and analysis of telemetry data from applications.\n",
    "\n",
    "You can use Application Insights to monitor telemetry from many kinds of application, including applications that are not running in Azure. All that's required is a low-overhead instrumentation package to capture and send the telemetry data to Application Insights. The necessary package is already included in Azure Machine Learning Web services, so you can use it to capture and review telemetry from models published with Azure Machine Learning.\n",
    "\n",
    "### Enable Application insights\n",
    "\n",
    "To log telemetry in application insights from an Azure machine learning service, you must **have an Application Insights resource associated with your Azure Machine Learning workspace**, and you must **configure your service to use it for telemetry logging**.\n",
    "\n",
    "When you create an Azure Machine Learning workspace, you can select an Azure Application Insights resource to associate with it. If you do not select an existing Application Insights resource, a new one is created in the same resource group as your workspace.\n",
    "\n",
    "```python\n",
    "# Find out the Application Insights associated with the workspace\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws.get_details()['applicationInsights']\n",
    "\n",
    "# Enable Application Insights for a service when deploying a new real-time service\n",
    "dep_config = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
    "                                                memory_gb = 1,\n",
    "                                                enable_app_insights=True) # enable Application Insights\n",
    "\n",
    "# Enable Application Insights for a service already deployed: modify the deployment configuration for Azure Kubernetes Service (AKS) based services in the Azure portal.\n",
    "service = ws.webservices['my-svc']\n",
    "service.update(enable_app_insights=True)\n",
    "```\n",
    "\n",
    "### Capture and view telemetry\n",
    "\n",
    "Application Insights automatically captures any information written to the standard output and error logs, and provides a query capability to view data in these logs.\n",
    "\n",
    "To capture telemetry data for Application insights, you can write any values to the standard output log in the scoring script for your service by using a **print statement (Note: not using log())**.\n",
    "\n",
    "```python\n",
    "def init():\n",
    "    global model\n",
    "    model = joblib.load(Model.get_model_path('my_model'))\n",
    "def run(raw_data):\n",
    "    data = json.loads(raw_data)['data']\n",
    "    predictions = model.predict(data)\n",
    "    log_txt = 'Data:' + str(data) + ' - Predictions:' + str(predictions)\n",
    "    print(log_txt)\n",
    "    return predictions.tolist()\n",
    "```\n",
    "\n",
    "To analyze captured log data, you can use the Log Analytics query interface for Application Insights in the Azure portal. This interface supports a SQL-like query syntax that you can use to extract fields from logged data, including custom dimensions created by your Azure Machine Learning service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 Monitor  data drift with Azure Machine Learning\n",
    "\n",
    "Changing trends in data over time can reduce the accuracy of the predictions made by a model. Monitoring for this data drift is an important way to ensure your model continues to predict accurately.\n",
    "\n",
    "### Monitor data drift by comparing datasets\n",
    "Azure Machine Learning supports data drift monitoring through the use of datasets. You can compare two registered datasets to detect data drift, or you can capture new feature data submitted to a deployed model service and compare it to the dataset with which the model was trained.\n",
    "\n",
    "To monitor data drift using registered datasets, you need to register two datasets:\n",
    "\n",
    "- A baseline dataset - usually the original training data.\n",
    "- A target dataset that will be compared to the baseline based on time intervals. This dataset requires a column for each feature you want to compare, and a timestamp column so the rate of data drift can be measured.\n",
    "\n",
    "```python\n",
    "# create dataset monitors\n",
    "from azureml.datadrift import DataDriftDetector\n",
    "\n",
    "monitor = DataDriftDetector.create_from_datasets(workspace=ws,\n",
    "                                                 name='dataset-drift-detector',\n",
    "                                                 baseline_data_set=train_ds,\n",
    "                                                 target_data_set=new_data_ds,\n",
    "                                                 compute_target='aml-cluster',\n",
    "                                                 frequency='Week',\n",
    "                                                 feature_list=['age','height', 'bmi'],\n",
    "                                                 latency=24)\n",
    "\n",
    "# backfill to immediately compare the baseline dataset to existing data in the target dataset\n",
    "import datetime as dt\n",
    "\n",
    "backfill = monitor.backfill( dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "```\n",
    "\n",
    "### Monitor data drift in service inference data\n",
    "\n",
    "```python\n",
    "# Register the baseline dataset with the model\n",
    "\n",
    "from azureml.core import Model, Dataset\n",
    "\n",
    "model = Model.register(workspace=ws,model_path='./model/model.pkl', model_name='my_model',    \n",
    "                       datasets=[(Dataset.Scenario.TRAINING, train_ds)])\n",
    "```\n",
    "To collect inference data for comparison, you must **enable data collection** for services in which the model is used. To do this, you must use the **ModelDataCollector** class in each service's ***scoring script***, **writing code to capture data and predictions and write them to the data collector** (which will store the collected data in Azure blob storage)\n",
    "\n",
    "```python\n",
    "from azureml.monitoring import ModelDataCollector\n",
    "\n",
    "def init():\n",
    "    global model, data_collect, predict_collect\n",
    "    model_name = 'my_model'\n",
    "    model = joblib.load(Model.get_model_path(model_name))\n",
    "\n",
    "    # Enable collection of data and predictions\n",
    "    data_collect = ModelDataCollector(model_name,\n",
    "                                      designation='inputs',\n",
    "                                      features=['age','height', 'bmi'])\n",
    "    predict_collect = ModelDataCollector(model_name,\n",
    "                                         designation='predictions',\n",
    "                                         features=['prediction'])\n",
    "def run(raw_data):\n",
    "    data = json.loads(raw_data)['data']\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # collect data and predictions\n",
    "    data_collect(data)\n",
    "    predict_collect(predictions)\n",
    "\n",
    "    return predictions.tolist()\n",
    "```\n",
    "\n",
    "With the data collection code in place in the scoring script, you can enable data collection in the deployment configuration:\n",
    "\n",
    "```python\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "dep_config = AksWebservice.deploy_configuration(collect_model_data=True)\n",
    "```\n",
    "\n",
    "Now that the baseline dataset is registered with the model, and the target data is being collected by deployed services, you can configure data drift monitoring by using a **DataDriftDetector** class:\n",
    "\n",
    "```python\n",
    "from azureml.datadrift import DataDriftDetector, AlertConfiguration\n",
    "\n",
    "# create a new DataDriftDetector object for the deployed model\n",
    "model = ws.models['my_model']\n",
    "datadrift = DataDriftDetector.create_from_model(ws, model.name, model.version,\n",
    "                                     services=['my-svc'],\n",
    "                                     frequency=\"Week\")\n",
    "```\n",
    "\n",
    "The data drift detector will run at the specified frequency, but you can run it on-demand as an experiment:\n",
    "\n",
    "```python\n",
    "from azureml.core import Experiment, Run\n",
    "from azureml.widgets import RunDetails\n",
    "import datetime as dt\n",
    "\n",
    "# or specify existing compute cluster\n",
    "run = datadrift.run(target_date=dt.today(),\n",
    "                    services=['my-svc'],\n",
    "                    feature_list=['age','height', 'bmi'],\n",
    "                    compute_target='aml-cluster')\n",
    "\n",
    "# show details of the data drift run\n",
    "exp = Experiment(ws, datadrift._id)\n",
    "dd_run = Run(experiment=exp, run_id=run.id)\n",
    "RunDetails(dd_run).show()\n",
    "```\n",
    "\n",
    "### Configure alerts\n",
    "\n",
    "```python\n",
    "alert_email = AlertConfiguration('data_scientists@contoso.com')\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'dataset-drift-detector', \n",
    "                                                 baseline_data_set, target_data_set,\n",
    "                                                 compute_target=cpu_cluster,\n",
    "                                                 frequency='Week', latency=2,\n",
    "                                                 drift_threshold=.3,\n",
    "                                                 alert_configuration=alert_email)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the whole workflow from building, deploying, consuming, to monitoring a model.\n",
    "- Refer to Jupyter Notebook for the complete codes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
