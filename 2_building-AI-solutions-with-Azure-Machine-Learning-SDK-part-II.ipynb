{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Note - Building AI Solutions with Azure Machine Learning\n",
    "This notebook collects the notes taken through the course of **[Build AI solutions with Azure Machine Learning](https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/)** offered by Microsoft, with supplements from the **[documentation of Azure Machine Learning SDK for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py)**.\n",
    "\n",
    "This notebook contains Labs 06 - 07 of the learning course, which correspond to \"Deploy and Consume Models\" section in the exam guideline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Deploy real-time machine learning services with Azure Machine Learning\n",
    "In machine learning, *inferencing* refers to the use of a trained model to predict labels for new data on which the model has not been trained. In Azure Machine learning, you can create **real-time inferencing solutions by deploying a model as a service**, hosted in a **containerized platform** such as **Azure Kubernetes Services (AKS)**.\n",
    "\n",
    "**Notes:** \n",
    "- **We can also deploy the model on Azure Container Instances (ACI) Web Service or local Docker-based service during development and testing.**\n",
    "- **ACI web service is best for small scale testing and quick deployments, and AKS is for deloyments as a production-scale web service.**\n",
    "\n",
    "To deploy a model as a real-time inferencing service, you must perform the following tasks:\n",
    "1.\tRegister a trained model\n",
    "2.\tDefine an inference configuration\n",
    "    1.\tCreate an **entry script**: The entry script receives data submitted to a deployed web service and passes it to the model. It then takes the response returned by the model and returns that to the client. *The script is specific to your model.* It must understand the data that the model expects and returns.\n",
    "        1.\t`init()`: Called when the service is initialized - Typically, this function loads the model into a global object. This function is run only once, when the Docker container for your web service is started.\n",
    "        2.\t`run(inpute_data)`: Called with new data is submitted to the service - This function uses the model to predict a value based on the input data. Inputs and outputs of the run typically use JSON for serialization and deserialization. You can also work with raw binary data. You can transform the data before sending it to the model or before returning it to the client.\n",
    "\n",
    "            ```python\n",
    "            [To include entry script codes]\n",
    "            ```\n",
    "        \n",
    "    2.\tCreate an environment\n",
    "    3.\tCombine the script and environment in an InferenceConfig\n",
    "\n",
    "    ```python\n",
    "    from azureml.core.model import InferenceConfig\n",
    "\n",
    "    classifier_inference_config = InferenceConfig(runtime= \"python\",\n",
    "                                                  source_directory = 'service_files',\n",
    "                                                  entry_script=\"score.py\",\n",
    "                                                  conda_file=\"env.yml\")\n",
    "    ```\n",
    "\n",
    "3.\tDefine a deployment configuration on the chosen compute target\n",
    "    - AksCompute\n",
    "    ```python\n",
    "    from azureml.core.compute import ComputeTarget, AksCompute\n",
    "    from azureml.core.webservice import AksWebservice\n",
    "    ```\n",
    "\n",
    "    - ACI deployment\n",
    "    ```python\n",
    "    from azureml.core.webservice import AciWebservice\n",
    "    ```\n",
    "    \n",
    "    - local Docker-based service\n",
    "    ```python\n",
    "    from azureml.core.webservice import LocalWebservice\n",
    "    ```\n",
    "4.\tDeploy the model\n",
    "```python\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name = 'classifier-service',\n",
    "                       models = [model], #1. Model registered\n",
    "                       inference_config = classifier_inference_config, # 2. Inference Configuration\n",
    "                       deployment_config = classifier_deploy_config, # 3. deployment configuration\n",
    "                       deployment_target = production_cluster) # (Optional) 3. deployment configuration\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)\n",
    "```\n",
    "\n",
    "To delete a deployed web service, use `service.delete()`. To delete a registered model, use `model.delete()`.\n",
    "\n",
    "#### [Additional Topic: Create an endpoint](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service#create-an-endpoint)\n",
    "\n",
    "**To create an endpoint, use `AksEndpoint.deploy_configuration` instead of `AksWebservice.deploy_configuration()`.**\n",
    "\n",
    "```python\n",
    "import azureml.core,\n",
    "from azureml.core.webservice import AksEndpoint\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "# select a created compute\n",
    "compute = ComputeTarget(ws, 'myaks')\n",
    "namespace_name= endpointnamespace\n",
    "# define the endpoint and version name\n",
    "endpoint_name = \"mynewendpoint\"\n",
    "version_name= \"versiona\"\n",
    "# create the deployment config and define the scoring traffic percentile for the first deployment\n",
    "endpoint_deployment_config = AksEndpoint.deploy_configuration(cpu_cores = 0.1, memory_gb = 0.2,\n",
    "                                                              enable_app_insights = True,\n",
    "                                                              tags = {'sckitlearn':'demo'},\n",
    "                                                              description = \"testing versions\",\n",
    "                                                              version_name = version_name,\n",
    "                                                              traffic_percentile = 20)\n",
    " # deploy the model and endpoint\n",
    " endpoint = Model.deploy(ws, endpoint_name, [model], inference_config, endpoint_deployment_config, compute)\n",
    " # Wait for he process to complete\n",
    " endpoint.wait_for_deployment(True)\n",
    "```\n",
    "\n",
    "To *consume* a deployed real-time service (or model or endpoint), we’ll need the following: **(Note: Recall the consume tab in AML Studio.)**\n",
    "-\tHTTP Post/ Url **(Note: Recall the step to copy the REST url on AML studio and paste it in the script.)**\n",
    "-\tKey **(Note: Recall the step to copy the primary key on AML studio and paste it in the script.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 Deploy batch inference pipelines with Azure Machine Learning\n",
    "The steps are not very consistent between lectures and lab codes. Refer to the lab codes when there’s inconsistency.\n",
    "1.\tRegister a model\n",
    "2.\tCreate a scoring script and define a run context that includes the dependencies required by the script\n",
    "3.\tCreate a pipeline with **ParallelRunStep** (As the chapter name suggests, we’re going to create a step in pipeline)\n",
    "```python\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "# Define the parallel run step step configuration\n",
    "# Create the parallel run step\n",
    "```\n",
    "4.\tRun the pipeline and retrieve the step output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
